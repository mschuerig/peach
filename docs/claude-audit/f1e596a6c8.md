---
commit: "f1e596a6c87f8688d3597db53204a3ffb84c633d"
date: "2026-02-23T02:17:33+01:00"
author: "Michael Sch√ºrig <michael@schuerig.de>"
subject: "Add technical research: sampled instrument NotePlayer implementation"
sessions: ["df545d3f-d645-43e6-a9ec-509720884d30"]
prev: "e5dfaa5c35.md"
next: "4bc42e4d61.md"
---

[‚Üê Previous (e5dfaa5c35)](e5dfaa5c35.md) | [Index](index.md) | [Next (4bc42e4d61) ‚Üí](4bc42e4d61.md)

# Commit f1e596a6c8

**Date:** 2026-02-23 02:17:33 UTC+01:00  
**Author:** Michael Sch√ºrig <michael@schuerig.de>

## Commit message

Add technical research: sampled instrument NotePlayer implementation

Evaluates approaches for adding sampled instrument playback to Peach
with fractional-cent pitch precision. Recommends AVAudioUnitSampler + SF2
SoundFont files for auto-discovery, scalability to 20+ instruments, and
future user-provided SF2 support.

Co-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>

## Changed files

```
...ed-instrument-noteplayer-research-2026-02-23.md | 745 +++++++++++++++++++++
 1 file changed, 745 insertions(+)
```

## Session `df545d3f‚Ä¶`

*2026-02-23 00:19:24 UTC ‚Üí 2026-02-23 01:17:36 UTC*

### üßë Human (00:19:24)

<command-message>bmad-bmm-technical-research</command-message>
<command-name>/bmad-bmm-technical-research</command-name>
<command-args>The aim is to implement another NotePlayer that can play sampled instruments. The goal is, to offer users a choice of instruments for training, in addition to the existing sine wave. There are a few considerations that I can think of

* It must be possible to play the instrument not just at exactly the sampled pitch. For the purpose of this app, the samples must be shifted by fractional cents from 0.1 to 100.0.
* It is probably enough to have just one articulation for each instrument.
* Use a SoundFont or individual files?
* Promising candidates 
  * https://github.com/peastman/sso
  * https://philharmonia.co.uk/resources/sound-samples/ (no piano)</command-args>

### ü§ñ Assistant (00:19:39)

Now let me create the starter output file and begin the scope confirmation.

### ü§ñ Assistant (00:20:17)

Welcome Michael! I understand you want to conduct **technical research** on implementing a **Sampled Instrument NotePlayer** for the Peach ear-training app.

From your brief, here's what I've captured:

**Research Topic:** Sampled Instrument NotePlayer Implementation

**Research Goals:**
- Implement a new `NotePlayer` that plays sampled instruments (alongside the existing sine wave)
- Support pitch shifting by fractional cents (0.1 to 100.0 cents from sampled pitch)
- Evaluate sound source format: SoundFont (.sf2) vs individual audio files
- Assess candidate sample libraries: [SSO (Sonatina Symphonic Orchestra)](https://github.com/peastman/sso) and [Philharmonia Sound Samples](https://philharmonia.co.uk/resources/sound-samples/)
- Single articulation per instrument is likely sufficient

**Technical Research Scope:**

- **Architecture Analysis** - How a sampled NotePlayer fits into the existing NotePlayer interface, audio pipeline design, sample loading strategy
- **Implementation Approaches** - Web Audio API sample playback, real-time pitch shifting with sub-cent precision, buffer management patterns
- **Technology Stack** - SoundFont parsing libraries, audio format considerations, Web Audio API capabilities for pitch shifting
- **Integration Patterns** - SoundFont (.sf2) vs individual files trade-offs, sample packaging/bundling, lazy loading strategies
- **Performance Considerations** - Memory footprint of samples, pitch-shifting algorithm accuracy at fractional cents, latency constraints for ear training

**Research Approach:**
- Current web data with rigorous source verification
- Multi-source validation for critical technical claims
- Confidence levels for uncertain technical information
- Comprehensive technical coverage with architecture-specific insights

**Does this technical research scope and approach align with your goals?**

**[C]** Continue - Begin technical research with this scope

### üßë Human (00:21:58)

Add to the research scope that we need to find suitable samples with a suitable license.

### ü§ñ Assistant (00:22:04)

Good point. I've added that as a dedicated research area. Here's the updated scope:

**Technical Research Scope:**

- **Architecture Analysis** - How a sampled NotePlayer fits into the existing NotePlayer interface, audio pipeline design, sample loading strategy
- **Implementation Approaches** - Web Audio API sample playback, real-time pitch shifting with sub-cent precision, buffer management patterns
- **Technology Stack** - SoundFont parsing libraries, audio format considerations, Web Audio API capabilities for pitch shifting
- **Integration Patterns** - SoundFont (.sf2) vs individual files trade-offs, sample packaging/bundling, lazy loading strategies
- **Performance Considerations** - Memory footprint of samples, pitch-shifting algorithm accuracy at fractional cents, latency constraints for ear training
- **Sample Sourcing & Licensing** - Identify high-quality, freely licensed instrument samples suitable for distribution with the app; evaluate candidate libraries (SSO, Philharmonia, and others discovered during research) for license compatibility, coverage, and quality

**Does this updated scope align with your goals?**

**[C]** Continue - Begin technical research with this scope

### üßë Human (00:22:24)

c

### ü§ñ Assistant (00:22:37)

Now let me load the next step.

### ü§ñ Assistant (00:22:45)

Scope confirmed. Now I'll conduct **technology stack analysis** for the Sampled Instrument NotePlayer to understand the technology landscape.

Let me search for current technology insights across several areas in parallel.

### ü§ñ Assistant (00:22:57)

Good initial results. Let me dig deeper into specific technical areas.

### ü§ñ Assistant (00:23:22)

Now let me also understand the existing codebase architecture before writing the analysis.

### ü§ñ Assistant (00:24:38)

Important discovery ‚Äî this is a **native Swift/iOS app** using `AVAudioEngine`, not a web app. Let me redirect my research to the correct platform.

### ü§ñ Assistant (00:26:20)

Now I have a comprehensive picture. Let me write the technology stack analysis to the document.

### ü§ñ Assistant (00:27:08)

I've completed the **technology stack analysis** for the Sampled Instrument NotePlayer. Here are the key findings:

**Key Technology Findings:**

- **AVAudioUnitTimePitch** is the most natural fit ‚Äî accepts pitch in cents as a Float, supports fractional values (0.1 cent precision), and integrates cleanly with the existing AVAudioPlayerNode pattern
- **AVAudioUnitSampler** is a viable alternative that natively loads SF2/DLS files, with MIDI pitch bend offering ~0.024 cents/step resolution ‚Äî but has known stability concerns with malformed SF2 files
- **Individual WAV files** loaded via AVAudioFile into AVAudioPCMBuffer best mirrors the existing `SineWaveNotePlayer` architecture
- **No external dependencies needed** ‚Äî Apple's native AVAudioEngine stack covers all requirements
- For shifts of 0.1‚Äì100 cents, pitch quality degradation is negligible with either approach

**Ready to proceed to integration patterns analysis?**

**[C]** Continue - Proceed to integration patterns (SoundFont vs individual files, sample packaging, architecture)

### üßë Human (00:46:48)

For consideration:
* In the Philharmonia library, the raw samples are MP3.
* In the Sonatina library, the raw samples are WAV and FLAC and there are gaps; not every note is sampled, only 4 samples per octave.

### ü§ñ Assistant (00:46:56)

Good observations ‚Äî those are important practical constraints. Let me note them:

**Philharmonia:**
- MP3 source format means lossy compression artifacts are baked in. For an ear training app focused on sub-cent pitch discrimination, this could be a concern ‚Äî MP3 encoding can introduce subtle pitch/phase artifacts. Transcoding MP3‚ÜíWAV doesn't recover lost information.

**SSO:**
- WAV/FLAC source (lossless) is much better for pitch-critical work.
- Only ~4 samples per octave means each sample must cover ~3 semitones (300 cents) in both directions via pitch shifting. Combined with the app's 0.1‚Äì100 cent training shifts, the total pitch displacement from the original sample could reach ~400 cents ‚Äî pushing into territory where `AVAudioUnitTimePitch` artifacts become audible.

These are significant trade-offs that will factor heavily into the integration patterns analysis. Let me capture them and proceed.

**[C]** Continue - Proceed to integration patterns analysis incorporating these constraints

### üßë Human (00:47:31)

c

### ü§ñ Assistant (00:47:42)

Now I'll conduct **integration patterns analysis** adapted to the specific integration questions for a sampled NotePlayer. Let me research the key decision points.

### ü§ñ Assistant (00:48:26)

Now let me write the integration patterns analysis. This is the critical decision-making section.

### ü§ñ Assistant (00:49:40)

I've completed the **integration patterns analysis**. Here are the key findings:

**Two Viable Approaches:**

- **Approach A (AVAudioPlayerNode + AVAudioUnitTimePitch + WAV files)** ‚Äî Mirrors existing SineWaveNotePlayer pattern, direct cent control, but carries a known TimePitch crackling risk on some iOS versions
- **Approach B (AVAudioUnitSampler + SF2)** ‚Äî Automatic sample mapping, no TimePitch crackling, but different API pattern and known crash risk with malformed SF2 files

**Critical Insights:**

- SSO's 4 samples/octave aligns with industry best practice (every major third). Worst-case combined shift (~250 cents) is within acceptable quality bounds
- Individual WAV files give more control and transparency; SF2 gives automatic mapping
- App bundle size for 3‚Äì5 instruments at selective density is manageable (~15‚Äì25 MB)
- On-Demand Resources is available if the library grows
- The AVAudioUnitTimePitch crackling issue is a real risk that needs prototyping to verify

**Ready to proceed to architectural patterns analysis?**

**[C]** Continue - Proceed to architectural patterns and design decisions

### üßë Human (00:51:50)

c

### ü§ñ Assistant (00:52:00)

Now I'll focus on **architectural patterns and design decisions** for the sampled NotePlayer. Let me research specific architectural concerns.

### ü§ñ Assistant (00:52:53)

Now let me write the architectural patterns analysis to the document.

### ü§ñ Assistant (00:53:58)

I've completed the **architectural patterns analysis**. Here's the key architectural finding:

**A third approach emerged as the strongest candidate:**

**Playback rate shifting** (no AVAudioUnitTimePitch node at all). By adjusting the playback rate via `pow(2, cents/1200)`, we get:
- Zero phase vocoder artifacts
- Completely avoids the iOS 16+ TimePitch crackling bug
- The simplest possible audio graph (identical to SineWaveNotePlayer)
- Duration compensation is straightforward (adjust buffer frame count)
- At worst case ~250 cents, timbre change is subtle and acceptable

**Key architectural decisions documented:**
- Individual WAV files (not SF2) ‚Äî transparent, testable, matches existing pattern
- Pre-loaded sample cache at init ‚Äî eliminates per-play disk I/O
- Same NotePlayer protocol ‚Äî zero changes to consumers
- Instrument enum + factory at composition root
- Bundle samples in app initially (~3.5 MB per instrument)

**Ready to proceed to implementation research?**

**[C]** Continue - Proceed to sample sourcing, licensing evaluation, and implementation details

### üßë Human (01:04:02)

I'm not comfortable with sticking to the design decisions embodied in the SineWaveNotePlayer, just because they are there. Back when we created that implementation, we didn't look at possible later requirements. Now it is later and we need to consider what design is best for our extended requirements. So, I would happily refactor the existing code if it makes the new code easier.
Another consideration: Although we are going to start with a single sampled instrument, there are going to be several, say 20, of them. I don't want to write individual code for each instrument, I don't even want to write a list of available instruments. Instead, I'd like to discover available instruments from sample file names or SoundFont metadata.

### ü§ñ Assistant (01:04:15)

Excellent points, Michael. Those are important design principles that change the analysis significantly:

1. **No sacred cows** ‚Äî the existing SineWaveNotePlayer pattern is not a constraint. If a different architecture is better for sampled instruments, we refactor.

2. **Convention over configuration** ‚Äî instruments should be discovered from the filesystem/bundle, not hardcoded in an enum. Drop a sample set into the right folder (or add an SF2 with presets) and it just appears in the UI.

3. **Scale to ~20 instruments** ‚Äî this shifts the bundle size math considerably. At 3.5 MB per instrument √ó 20 = ~70 MB of WAV files. An SF2 containing 20 instruments with shared sample zones could be significantly smaller due to sample reuse.

These observations actually tip the balance back toward **AVAudioUnitSampler + SF2** more strongly than I initially assessed:

- **Auto-discovery**: SF2 files contain named presets with program numbers ‚Äî the app can enumerate all available instruments from the file's metadata. No hardcoded list needed.
- **20 instruments in one file**: SF2 shares sample data across presets (e.g., the same piano samples can be reused). Much more space-efficient than 20 sets of individual WAV files.
- **No per-instrument code**: One `SampledNotePlayer` class handles any instrument from any SF2 preset ‚Äî the preset number is just a parameter.
- **Pitch shifting via MIDI pitch bend**: Avoids the TimePitch crackling issue entirely.

The refactoring angle also makes **Approach B (AVAudioUnitSampler)** more attractive, since we're no longer constrained to match AVAudioPlayerNode patterns.

Let me capture these revised considerations and proceed to the implementation research with this updated framing.

**[C]** Continue - Proceed to implementation research with revised design direction

### üßë Human (01:05:59)

As a future option, we may even offer users to use their own samples. That's probably much easier with SF2.

### ü§ñ Assistant (01:06:07)

Great point. That seals the case for SF2 even further:

- **User-provided SF2 files** are a well-understood concept in the music world. Users can find thousands of free SF2 files online, or export them from DAWs. The format is a de facto standard.
- **Import UX is simple**: one file, dropped in via Files app or share sheet. No need to explain folder structures or naming conventions.
- **Validation is straightforward**: load the SF2, enumerate presets, present them. If it loads, it works.
- **With individual WAV files**, user-provided instruments would require defining a folder structure, naming convention, pitch mapping ‚Äî a whole specification that users need to learn and follow.

This also raises the stability concern about `AVAudioUnitSampler` crashing on malformed SF2 files to a higher priority ‚Äî user-provided files are unpredictable. We'll need to investigate whether we can validate SF2 files before passing them to the sampler, or whether there's a more robust SF2 renderer.

Let me carry all of this forward into the implementation research.

**[C]** Continue - Proceed to implementation research

### üßë Human (01:08:08)

Crashes caused by malformed SF2 files are not overly dramatic, as long as we can recover from them and offer the user to delete the file.

### ü§ñ Assistant (01:08:15)

Agreed. A crash-and-recover pattern is pragmatic:

- Track which SF2 was last loaded (UserDefaults flag before loading, clear after success)
- On next launch, if the flag is still set, the previous SF2 caused a crash ‚Äî offer to remove it
- Built-in SF2 files are curated and tested, so crashes would only come from user imports

That de-risks AVAudioUnitSampler enough to proceed confidently. Let me move to implementation research now.

### ü§ñ Assistant (01:08:24)

Now I'll conduct the **implementation research** with the revised direction: AVAudioUnitSampler + SF2, auto-discovery, scale to 20+ instruments, future user-provided SF2 support.

### üßë Human (01:09:20)

[Request interrupted by user for tool use]

### üßë Human (01:09:51)

When you look at Sonatina and alternatives, keep in mind that they contain multiple articulations that we don't need. That also means, we need a way to create a pruned SF2.

### ü§ñ Assistant (01:09:59)

Good point. For Peach we only need one articulation per instrument (sustained/normal). Shipping all articulations wastes bundle size and clutters the instrument list during auto-discovery.

So the pipeline becomes:

1. **Source**: SSO (or other library) with full articulations
2. **Prune**: Remove unwanted presets (staccato, pizzicato, tremolo, etc.) ‚Äî keep only the "sustain" variant per instrument
3. **Package**: Export as a lean SF2 with only the needed presets

**Polyphone** can do this manually (open SF2, delete unwanted presets, save). But for reproducibility and future updates, a scriptable approach would be better.

Let me research scriptable SF2 pruning and also factor this into the implementation research.

### ü§ñ Assistant (01:10:22)

Good ‚Äî so we need a **reproducible SF2 build pipeline** as a tool script. Let me write the full implementation research now.

### ü§ñ Assistant (01:10:26)

Now let me update the architectural section to reflect the revised direction and write the implementation research.

### ü§ñ Assistant (01:11:38)

Now let me append the implementation research section.

### ü§ñ Assistant (01:13:05)

I've completed the **implementation research**. Here are the highlights:

**Sample Library Evaluation:**

| Library | Format | License | Size | Piano? | Verdict |
|---------|--------|---------|------|--------|---------|
| **GeneralUser GS** | SF2 | Free (any use) | 30 MB | Yes | **Phase 1 default** ‚Äî best value |
| **Salamander Piano** | WAV/SFZ | Public domain | ~500 MB raw | Yes (only) | Phase 2 piano upgrade |
| **Iowa MIS** | AIFF | Unrestricted | Varies | Yes | Phase 2 orchestral upgrade |
| **SSO** | WAV/FLAC/SF2 | CC Sampling Plus | ~400 MB full | Yes | Phase 2 supplement |
| **Philharmonia** | MP3 | CC-BY-SA | Varies | No | **Not recommended** (lossy) |

**Implementation Roadmap:**
1. Prototype SoundFontNotePlayer with AVAudioUnitSampler
2. Implement SF2 preset auto-discovery
3. Prune & bundle GeneralUser GS (~20 key instruments)
4. Refactor NotePlayer protocol if needed
5. User-provided SF2 support (future)

**SF2 Pruning Pipeline:** Use Polyphone GUI with documented runbook in `tools/build-sf2.md`. Automation via Python later if needed.

**Ready to proceed to the final synthesis?**

**[C]** Continue - Produce final synthesis and executive summary

### üßë Human (01:14:31)

c

### ü§ñ Assistant (01:14:44)

Now I'll produce the final synthesis ‚Äî an executive summary, table of contents, and conclusion to wrap the research document into a complete deliverable. The document already contains all substantive research; I'll add the framing.

### ü§ñ Assistant (01:15:25)

Now let me add the conclusion at the end of the document.

### ü§ñ Assistant (01:15:50)

The **technical research document is complete**. Here's what was produced:

**Document:** `docs/planning-artifacts/research/technical-sampled-instrument-noteplayer-research-2026-02-23.md`

**Structure:**
- Executive Summary with key findings and top 5 recommendations
- Table of Contents linking all sections
- Research Methodology
- Technology Stack Analysis (platform, pitch shifting options, SF2 parsers, formats)
- Integration Patterns (Approach A vs B comparison, SF2 vs WAV, quality assessment, bundle size)
- Architectural Patterns (revised for SF2 approach, auto-discovery, pitch bend, pruning pipeline, crash recovery)
- Implementation & Sample Sourcing (5 libraries evaluated, 3-phase strategy, roadmap, testing, risks)
- Conclusion with requirements coverage matrix
- Full source bibliography (13 key references)

**The bottom line:** Use `AVAudioUnitSampler` + SF2 SoundFont files. Start with GeneralUser GS (30 MB, pruned to ~20 instruments). Prototype first to validate pitch accuracy and latency.

**[C]** Complete - Finalize the research

### üßë Human (01:16:00)

c

### ü§ñ Assistant (01:16:09)

Research complete. The document is at:

`docs/planning-artifacts/research/technical-sampled-instrument-noteplayer-research-2026-02-23.md`

To summarize the journey: we started evaluating Web Audio API approaches before discovering this is a native Swift app, pivoted to AVAudioEngine options, explored three pitch-shifting approaches, and landed on **AVAudioUnitSampler + SF2** after your input about scaling to 20+ instruments, auto-discovery, and future user-provided files. That feedback fundamentally changed the recommendation ‚Äî the earlier WAV-file approaches wouldn't have scaled.

**When you're ready to move forward, the prototype step is:**
1. Download GeneralUser GS SF2
2. Build a minimal `SoundFontNotePlayer` that loads it, plays a note with `startNote()`, and shifts pitch with `sendPitchBend()`
3. Measure pitch accuracy with a tuner and compare playback latency against the sine wave player

That will validate (or challenge) the core assumptions before committing to the full architecture.

### üßë Human (01:17:22)

commit this

### ü§ñ Assistant (01:17:36)

Committed as `f1e596a`.

---

[‚Üê Previous (e5dfaa5c35)](e5dfaa5c35.md) | [Index](index.md) | [Next (4bc42e4d61) ‚Üí](4bc42e4d61.md)
