---
commit: "a14387c1eebb69c1d34cc6cd4591f22d2e5d6728"
date: "2026-02-12T15:35:26+01:00"
author: "Michael Sch√ºrig <michael@schuerig.de>"
subject: "Add epics document and simplify architecture data model"
sessions: ["1d9dad18-d536-43fc-add5-2e922cfb966f", "383714ff-7c12-40fc-88ab-0c43777ffc40"]
prev: "df955d28ff.md"
next: "0a89329199.md"
---

[‚Üê Previous (df955d28ff)](df955d28ff.md) | [Index](index.md) | [Next (0a89329199) ‚Üí](0a89329199.md)

# Commit a14387c1ee

**Date:** 2026-02-12 15:35:26 UTC+01:00  
**Author:** Michael Sch√ºrig <michael@schuerig.de>

## Commit message

Add epics document and simplify architecture data model

Remove unnecessary note1CentOffset field from comparison record
model since the first note is always an exact MIDI note.

Co-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>

## Changed files

```
docs/planning-artifacts/architecture.md |   4 +-
 docs/planning-artifacts/epics.md        | 804 ++++++++++++++++++++++++++++++++
 2 files changed, 806 insertions(+), 2 deletions(-)
```

## Session 1 (`1d9dad18‚Ä¶`)

*2026-02-12 13:59:22 UTC ‚Üí 2026-02-12 14:32:44 UTC*

### üßë Human (13:59:22)

<command-message>bmad-bmm-create-epics-and-stories</command-message>
<command-name>/bmad-bmm-create-epics-and-stories</command-name>

### ü§ñ Assistant (13:59:31)

Now let me load and follow Step 1.

### ü§ñ Assistant (13:59:36)

Welcome Michael! Let's create comprehensive epics and stories for **Peach**. I'll be working with you as a product strategist and technical specifications writer to transform your planning documents into actionable, implementation-ready stories.

Let me first validate that all required input documents exist.

### ü§ñ Assistant (13:59:43)

All three documents found:

1. **PRD.md** - `/docs/planning-artifacts/prd.md`
2. **Architecture.md** - `/docs/planning-artifacts/architecture.md`
3. **UX Design Specification.md** - `/docs/planning-artifacts/ux-design-specification.md`

Before I proceed with extracting requirements from these documents, are there any other documents you'd like me to include in the analysis, or should any of these be excluded?

### üßë Human (14:01:18)

Use those you found and additionally docs/planning-artifacts/glossary.md

### ü§ñ Assistant (14:01:22)

Got it! Let me read all four documents now.

### ü§ñ Assistant (14:01:39)

All documents loaded. Let me now extract and organize all requirements, then initialize the output file.

### ü§ñ Assistant (14:03:02)

The output file has been initialized. Here's a summary of what was extracted:

**Functional Requirements Extracted: 44** (FR1‚ÄìFR43, including FR7a)
- Training Loop: FR1‚ÄìFR8 (+ FR7a)
- Adaptive Algorithm: FR9‚ÄìFR15
- Audio Engine: FR16‚ÄìFR20
- Profile & Statistics: FR21‚ÄìFR26
- Data Persistence: FR27‚ÄìFR29
- Settings: FR30‚ÄìFR36
- Localization & Accessibility: FR37‚ÄìFR38
- Device & Platform: FR39‚ÄìFR42
- Info Screen: FR43

**Non-Functional Requirements Extracted: 12** (NFR1‚ÄìNFR12)
- Performance: NFR1‚ÄìNFR5
- Accessibility: NFR6‚ÄìNFR9
- Data Integrity: NFR10‚ÄìNFR12

**Additional Requirements:**
- **From Architecture (14 items):** Xcode project setup as starter template (Epic 1 Story 1), protocol-based service architecture, SwiftData persistence, AVAudioEngine audio, Swift Testing, typed error enums, implementation sequence guidance, and more
- **From UX Design (12 items):** Liquid Glass design, stock SwiftUI, hub-and-spoke navigation, Training Screen button sizing, Feedback Indicator spec, profile visualization approach, empty states, Settings form pattern, no-gamification constraint, sensory hierarchy

Do these extracted requirements accurately represent what needs to be built? Any additions or corrections?

**Confirm the Requirements are complete and correct to [C] continue:**

### üßë Human (14:05:38)

c

### ü§ñ Assistant (14:06:15)

Now let me design the epic structure. Looking at the 44 FRs and user journeys, here are the natural user-value groupings I see for Peach:

---

**Proposed Epic Structure:**

### Epic 1: Project Foundation & Core Data
Establish the Xcode project, folder structure, and data persistence layer so that training data can be reliably stored and retrieved.
**FRs covered:** FR27, FR28, FR29
**Why standalone:** Everything depends on being able to store and retrieve comparison records. This delivers the foundation.

### Epic 2: Hear and Compare ‚Äî Core Audio Engine
Users can hear precisely generated tones with clean envelopes, enabling the fundamental listening experience.
**FRs covered:** FR16, FR17, FR18, FR19, FR20
**Why standalone:** Delivers working audio independent of the training loop. Can be validated in isolation.

### Epic 3: Train Your Ear ‚Äî The Comparison Loop
Users can start training immediately and answer comparisons in a rapid, reflexive loop with instant feedback. This is the core product experience.
**FRs covered:** FR1, FR2, FR3, FR4, FR5, FR6, FR7, FR7a, FR8, FR42
**Why standalone:** With Epics 1+2 in place, this delivers the complete training interaction ‚Äî the heart of Peach.

### Epic 4: Smart Training ‚Äî Adaptive Algorithm
The system intelligently selects comparisons based on the user's strengths and weaknesses, with cold start behavior for new users and continuous profile adaptation.
**FRs covered:** FR9, FR10, FR11, FR12, FR13, FR14, FR15
**Why standalone:** Upgrades the training loop from random comparisons to intelligent, personalized training.

### Epic 5: See Your Progress ‚Äî Profile & Statistics
Users can view their perceptual profile as a visualization, see summary statistics with trends, and access a profile preview from the Start Screen.
**FRs covered:** FR21, FR22, FR23, FR24, FR25, FR26
**Why standalone:** Delivers the "is this working?" experience ‚Äî users see their hearing mapped and improving.

### Epic 6: Make It Yours ‚Äî Settings & Configuration
Users can customize the training experience: algorithm behavior, note range, duration, reference pitch, and sound source selection.
**FRs covered:** FR30, FR31, FR32, FR33, FR34, FR35, FR36
**Why standalone:** All settings have sensible defaults from earlier epics; this adds user control.

### Epic 7: Polish & Ship ‚Äî Platform, Localization & Info
Users can use the app in English or German, on iPhone and iPad, in both orientations, with accessibility support and an Info Screen.
**FRs covered:** FR37, FR38, FR39, FR40, FR41, FR43
**Why standalone:** Takes a working app and makes it complete for release.

---

**FR Coverage Map:**

| FR | Epic | Description |
|---|---|---|
| FR1 | Epic 3 | Start training with single tap |
| FR2 | Epic 3 | Hear two sequential notes |
| FR3 | Epic 3 | Answer higher or lower |
| FR4 | Epic 3 | Immediate visual feedback |
| FR5 | Epic 3 | Haptic feedback on incorrect |
| FR6 | Epic 3 | Stop training via navigation |
| FR7 | Epic 3 | Discard incomplete comparisons |
| FR7a | Epic 3 | Return to Start Screen on foreground |
| FR8 | Epic 3 | Button disable/enable during notes |
| FR9 | Epic 4 | Profile-based comparison selection |
| FR10 | Epic 4 | Difficulty adjustment on correctness |
| FR11 | Epic 4 | Natural vs. Mechanical balance |
| FR12 | Epic 4 | Cold start at 100 cents |
| FR13 | Epic 4 | Profile continuity across sessions |
| FR14 | Epic 4 | Fractional cent precision |
| FR15 | Epic 4 | Expose algorithm parameters |
| FR16 | Epic 2 | Generate precise sine waves |
| FR17 | Epic 2 | Smooth attack/release envelopes |
| FR18 | Epic 2 | Same timbre for both notes |
| FR19 | Epic 2 | Configurable note duration |
| FR20 | Epic 2 | Configurable reference pitch |
| FR21 | Epic 5 | Profile visualization with keyboard + band |
| FR22 | Epic 5 | Profile Preview on Start Screen |
| FR23 | Epic 5 | Navigate to full Profile Screen |
| FR24 | Epic 5 | Summary statistics (mean, std dev) |
| FR25 | Epic 5 | Statistics trend indicator |
| FR26 | Epic 5 | Compute profile from stored data |
| FR27 | Epic 1 | Store comparison records |
| FR28 | Epic 1 | Local on-device persistence |
| FR29 | Epic 1 | Data integrity across restarts |
| FR30 | Epic 6 | Natural vs. Mechanical slider |
| FR31 | Epic 6 | Note range configuration |
| FR32 | Epic 6 | Note duration configuration |
| FR33 | Epic 6 | Reference pitch configuration |
| FR34 | Epic 6 | Sound source selection |
| FR35 | Epic 6 | Settings persistence |
| FR36 | Epic 6 | Immediate settings application |
| FR37 | Epic 7 | English + German localization |
| FR38 | Epic 7 | Basic accessibility support |
| FR39 | Epic 7 | iPhone and iPad support |
| FR40 | Epic 7 | Portrait and landscape |
| FR41 | Epic 7 | iPad windowed/compact mode |
| FR42 | Epic 3 | One-handed, large tap targets |
| FR43 | Epic 7 | Info Screen |

All 44 FRs are covered. NFRs are cross-cutting and will be addressed within the relevant epics (audio latency in Epic 2, data integrity in Epic 1, accessibility in Epic 7, etc.).

---

Does this epic structure align with your product vision? Should we adjust any groupings or the ordering? Are there natural dependencies we've missed?

**Do you approve this epic structure for proceeding to story creation?**

**Select an Option:** [A] Advanced Elicitation [P] Party Mode [C] Continue

### üßë Human (14:06:39)

c

### ü§ñ Assistant (14:06:44)

Let me save the approved epic structure to the document and proceed.

### ü§ñ Assistant (14:07:04)

Now loading the next step.

### ü§ñ Assistant (14:07:25)

Now let's begin creating stories, epic by epic. We'll start with **Epic 1**.

---

## Epic 1: Project Foundation & Core Data

**Goal:** Establish the Xcode project, folder structure, and data persistence layer so that training data can be reliably stored and retrieved.

**FRs covered:** FR27, FR28, FR29
**Relevant NFRs:** NFR10 (crash resilience), NFR11 (atomic writes), NFR12 (no migration data loss)
**Relevant Architecture:** Xcode 26.3 project setup, SwiftData, ComparisonRecord @Model, TrainingDataStore, feature-based folder structure, test target, Swift Testing framework

Here's my proposed story breakdown for Epic 1:

---

### Story 1.1: Create Xcode Project and Folder Structure

As a **developer**,
I want a properly configured Xcode project with the defined folder structure and test target,
So that all subsequent development has a consistent, organized foundation.

**Acceptance Criteria:**

**Given** no existing Xcode project
**When** the project is created
**Then** it uses Xcode 26.3 iOS App template with SwiftUI lifecycle, Swift language, and SwiftData storage
**And** the deployment target is iOS 26
**And** the folder structure matches the architecture document (App/, Core/Audio/, Core/Algorithm/, Core/Data/, Core/Profile/, Training/, Profile/, Settings/, Start/, Info/, Resources/)
**And** a PeachTests/ test target exists mirroring the source structure
**And** Swift Testing framework is configured for the test target
**And** the project builds and runs successfully on the simulator

---

### Story 1.2: Implement ComparisonRecord Data Model and TrainingDataStore

As a **developer**,
I want a persisted data model for comparison records with a store that supports create and read operations,
So that training results can be reliably stored and retrieved.

**Acceptance Criteria:**

**Given** the Xcode project from Story 1.1
**When** a ComparisonRecord is created
**Then** it contains fields: note1 (MIDI note), note1CentOffset, note2 (MIDI note), note2CentOffset, isCorrect (Bool), timestamp (Date)
**And** it is a SwiftData @Model

**Given** a TrainingDataStore instance
**When** a comparison record is saved
**Then** the record persists across app restarts
**And** the write is atomic ‚Äî no partial records are stored

**Given** a TrainingDataStore with stored records
**When** all records are fetched
**Then** every previously stored record is returned with all fields intact
**And** records survive app crashes, force quits, and unexpected termination

**Given** the TrainingDataStore
**When** unit tests are run using Swift Testing (@Test, #expect())
**Then** all CRUD operations are verified
**And** a typed DataStoreError enum exists for error cases

---

Does this story breakdown for Epic 1 capture the requirements correctly? Are the stories appropriately scoped and the acceptance criteria complete? Any adjustments before we move on to Epic 2?

### üßë Human (14:11:53)

note1CentOffset is not needed and this must be corrected in all documents. The first note is always an exact MIDI note.

### ü§ñ Assistant (14:12:05)

Good catch ‚Äî the architecture already states "the first note is always an exact MIDI note (cent offset = 0)" so the field is redundant. Let me fix it in both documents.

### ü§ñ Assistant (14:12:17)

The epics.md story hasn't been appended to the file yet ‚Äî it was only presented for review. I'll incorporate the correction when we write the stories. Let me also check the architecture for the `note1CentOffset` reference in the glossary definition of ComparisonRecord.

### ü§ñ Assistant (14:12:24)

Clean ‚Äî that was the only reference. Architecture is now corrected.

Here's the updated Story 1.2 with the fix:

---

### Story 1.2: Implement ComparisonRecord Data Model and TrainingDataStore

As a **developer**,
I want a persisted data model for comparison records with a store that supports create and read operations,
So that training results can be reliably stored and retrieved.

**Acceptance Criteria:**

**Given** the Xcode project from Story 1.1
**When** a ComparisonRecord is created
**Then** it contains fields: note1 (MIDI note), note2 (MIDI note), note2CentOffset, isCorrect (Bool), timestamp (Date)
**And** it is a SwiftData @Model

**Given** a TrainingDataStore instance
**When** a comparison record is saved
**Then** the record persists across app restarts
**And** the write is atomic ‚Äî no partial records are stored

**Given** a TrainingDataStore with stored records
**When** all records are fetched
**Then** every previously stored record is returned with all fields intact
**And** records survive app crashes, force quits, and unexpected termination

**Given** the TrainingDataStore
**When** unit tests are run using Swift Testing (@Test, #expect())
**Then** all CRUD operations are verified
**And** a typed DataStoreError enum exists for error cases

---

Epic 1 complete with 2 stories covering FR27, FR28, FR29 and NFR10, NFR11, NFR12. Good to proceed to Epic 2?

### üßë Human (14:12:46)

yes

### ü§ñ Assistant (14:13:04)

## Epic 2: Hear and Compare ‚Äî Core Audio Engine

**Goal:** Users can hear precisely generated tones with clean envelopes, enabling the fundamental listening experience.

**FRs covered:** FR16, FR17, FR18, FR19, FR20
**Relevant NFRs:** NFR1 (audio latency < 10ms), NFR3 (frequency precision to 0.1 cent)
**Relevant Architecture:** AVAudioEngine + AVAudioSourceNode, protocol-based NotePlayer, SineWaveNotePlayer implementation, 64-sample buffer at 44.1kHz, typed AudioError enum

Here's my proposed story breakdown:

---

### Story 2.1: Implement NotePlayer Protocol and SineWaveNotePlayer

As a **musician using Peach**,
I want to hear clean, precisely tuned sine wave tones,
So that I can train my pitch discrimination with accurate audio.

**Acceptance Criteria:**

**Given** a NotePlayer protocol
**When** it is defined
**Then** it exposes a method to play a note at a given frequency with a specified duration and envelope
**And** it exposes a method to stop playback

**Given** a SineWaveNotePlayer (implementing NotePlayer)
**When** a note is played at a target frequency
**Then** the generated tone is accurate to within 0.1 cent of the target frequency
**And** audio latency from trigger to audible output is < 10ms
**And** the implementation uses AVAudioEngine + AVAudioSourceNode

**Given** a SineWaveNotePlayer
**When** a note is played
**Then** it has a smooth attack/release envelope with no audible clicks or artifacts

**Given** two notes played in sequence
**When** both are rendered
**Then** they use the same timbre (sine wave)

**Given** the SineWaveNotePlayer
**When** unit tests are run
**Then** frequency generation accuracy is verified
**And** a typed AudioError enum exists for error cases (e.g., AudioError.engineStartFailed)

---

### Story 2.2: Support Configurable Note Duration and Reference Pitch

As a **musician using Peach**,
I want notes to play at a configurable duration and tuning standard,
So that the training experience matches my preferences.

**Acceptance Criteria:**

**Given** a NotePlayer
**When** a note duration is specified
**Then** the note plays for exactly that duration (with envelope attack/release within the duration)

**Given** a default configuration
**When** no reference pitch is set
**Then** frequencies are derived from A4 = 440Hz

**Given** a configurable reference pitch
**When** a different reference pitch value is provided (e.g., A4 = 442Hz)
**Then** all generated frequencies are derived from the new reference pitch

**Given** a MIDI note number and a cent offset
**When** a frequency is calculated
**Then** the frequency is mathematically derived from the reference pitch using standard equal temperament with the cent offset applied
**And** fractional cent precision (0.1 cent resolution) is supported

---

Epic 2 complete with 2 stories covering FR16, FR17, FR18, FR19, FR20 and NFR1, NFR3. Does this capture the audio engine requirements correctly? Any adjustments before we move to Epic 3?

### üßë Human (14:15:27)

yes

### ü§ñ Assistant (14:16:02)

## Epic 3: Train Your Ear ‚Äî The Comparison Loop

**Goal:** Users can start training immediately and answer comparisons in a rapid, reflexive loop with instant feedback. This is the core product experience.

**FRs covered:** FR1, FR2, FR3, FR4, FR5, FR6, FR7, FR7a, FR8, FR42
**Relevant NFRs:** NFR2 (no delay between comparisons), NFR4 (launch to interactive < 2s), NFR8 (44x44pt tap targets), NFR9 (haptic feedback)
**Relevant Architecture:** TrainingSession state machine (idle ‚Üí playingNote1 ‚Üí playingNote2 ‚Üí awaitingAnswer ‚Üí showingFeedback ‚Üí loop), @Observable, TrainingSession as error boundary
**Relevant UX:** Hub-and-spoke navigation, NavigationStack, large thumb-friendly buttons, Feedback Indicator (SF Symbols thumbs up/down, ~300-500ms), sensory hierarchy, navigation-as-stop, eyes-closed operation

Here's my proposed story breakdown:

---

### Story 3.1: Start Screen and Navigation Shell

As a **musician using Peach**,
I want to see a Start Screen when I open the app with a prominent Start Training button,
So that I can begin training immediately with a single tap.

**Acceptance Criteria:**

**Given** the app is launched
**When** the Start Screen appears
**Then** it displays a prominent Start Training button (`.borderedProminent`)
**And** it displays navigation buttons for Settings, Profile, and Info (icon-only, SF Symbols)
**And** it displays a placeholder area for the Profile Preview (to be implemented in Epic 5)
**And** the Start Screen is interactive within 2 seconds of app launch

**Given** the Start Screen
**When** the user taps Start Training
**Then** the Training Screen is presented via NavigationStack

**Given** the Start Screen
**When** the user taps Settings, Profile, or Info
**Then** the corresponding screen is navigated to (placeholder screens for now)
**And** each returns to the Start Screen when dismissed

**Given** the navigation structure
**When** any secondary screen is dismissed
**Then** the user always returns to the Start Screen (hub-and-spoke)

---

### Story 3.2: TrainingSession State Machine and Comparison Loop

As a **musician using Peach**,
I want to hear two notes in sequence and answer whether the second was higher or lower,
So that I can train my pitch discrimination through rapid comparisons.

**Acceptance Criteria:**

**Given** a TrainingSession (@Observable)
**When** it is initialized
**Then** it coordinates NotePlayer, TrainingDataStore, and a comparison source (using random comparisons at 100 cents as a temporary placeholder until Epic 4)
**And** it progresses through states: idle ‚Üí playingNote1 ‚Üí playingNote2 ‚Üí awaitingAnswer ‚Üí showingFeedback ‚Üí loop

**Given** the Training Screen is displayed
**When** training starts
**Then** the first comparison begins immediately ‚Äî no countdown, no transition animation
**And** the first note plays, followed by the second note

**Given** the user answers a comparison
**When** the answer is recorded
**Then** the result is written to TrainingDataStore
**And** the next comparison begins immediately with no perceptible delay

**Given** the TrainingSession
**When** a service error occurs (audio failure, data write failure)
**Then** the TrainingSession handles it gracefully ‚Äî the user never sees an error screen
**And** audio failure stops training silently
**And** data write failure logs internally but training continues

**Given** the TrainingSession
**When** unit tests are run
**Then** all state transitions are verified using mock protocol implementations

---

### Story 3.3: Training Screen UI with Higher/Lower Buttons and Feedback

As a **musician using Peach**,
I want large, thumb-friendly Higher/Lower buttons with immediate visual and haptic feedback,
So that I can train reflexively, even one-handed and with eyes closed.

**Acceptance Criteria:**

**Given** the Training Screen
**When** it is displayed
**Then** it shows Higher and Lower buttons that are large, thumb-friendly, and exceed 44x44pt minimum tap targets
**And** it shows Settings and Profile navigation buttons (visually subordinate to Higher/Lower)
**And** it uses stock SwiftUI components with no custom styles

**Given** the first note is playing
**When** the user looks at the Higher/Lower buttons
**Then** they are disabled (stock SwiftUI `.disabled()` appearance)

**Given** the second note begins playing
**When** the buttons become enabled
**Then** the user can tap Higher or Lower at any point during or after the second note

**Given** the user taps Higher or Lower
**When** the answer is submitted
**Then** both buttons disable immediately to prevent double-tap
**And** a Feedback Indicator appears: thumbs up (SF Symbol, system green) for correct, thumbs down (SF Symbol, system red) for incorrect
**And** if incorrect, a single haptic tick fires simultaneously (`UIImpactFeedbackGenerator`)
**And** if correct, no haptic (silence = confirmation)
**And** the Feedback Indicator displays for ~300-500ms then clears
**And** the next comparison begins

---

### Story 3.4: Training Interruption and App Lifecycle Handling

As a **musician using Peach**,
I want training to stop cleanly when I navigate away or leave the app,
So that no data is lost and I return to the Start Screen seamlessly.

**Acceptance Criteria:**

**Given** training is in progress
**When** the user taps Settings or Profile on the Training Screen
**Then** training stops immediately
**And** any incomplete comparison is silently discarded
**And** the user navigates to the selected screen
**And** that screen returns to the Start Screen when dismissed

**Given** training is in progress
**When** the app is backgrounded (home button, app switcher, phone call, headphone disconnect)
**Then** training stops and the incomplete comparison is discarded

**Given** the app was backgrounded during training
**When** the app is foregrounded
**Then** the user is returned to the Start Screen (not the Training Screen)

**Given** training is in the showingFeedback state
**When** the app is backgrounded
**Then** the already-answered comparison was already saved ‚Äî no data loss

---

Epic 3 complete with 4 stories covering FR1, FR2, FR3, FR4, FR5, FR6, FR7, FR7a, FR8, FR42 and NFR2, NFR4, NFR8, NFR9. Does this capture the training loop correctly? Any adjustments before we move to Epic 4?

### üßë Human (14:18:00)

yes

### ü§ñ Assistant (14:18:25)

## Epic 4: Smart Training ‚Äî Adaptive Algorithm

**Goal:** The system intelligently selects comparisons based on the user's strengths and weaknesses, with cold start behavior for new users and continuous profile adaptation.

**FRs covered:** FR9, FR10, FR11, FR12, FR13, FR14, FR15
**Relevant Architecture:** NextNoteStrategy protocol, AdaptiveNoteStrategy implementation, PerceptualProfile (in-memory, indexed by MIDI note 0‚Äì127, loaded from all comparisons on startup, updated incrementally), dependency on TrainingDataStore

---

### Story 4.1: Implement PerceptualProfile

As a **musician using Peach**,
I want the app to build and maintain an accurate map of my pitch discrimination ability,
So that training targets my actual weaknesses.

**Acceptance Criteria:**

**Given** a PerceptualProfile
**When** it is initialized
**Then** it holds aggregate statistics (arithmetic mean, standard deviation of detection thresholds) indexed by MIDI note (0‚Äì127)

**Given** a TrainingDataStore with existing comparison records
**When** the app starts
**Then** the PerceptualProfile is loaded by aggregating all stored records into per-note statistics

**Given** a new comparison result is recorded during training
**When** the PerceptualProfile is updated
**Then** it updates incrementally (not by re-aggregating all records)

**Given** a PerceptualProfile
**When** queried for weak spots
**Then** it identifies notes with the largest detection thresholds (poorest discrimination)

**Given** the PerceptualProfile
**When** no data exists for a MIDI note
**Then** that note is treated as a weak spot (cold start assumption)

**Given** the PerceptualProfile
**When** unit tests are run
**Then** aggregation, incremental update, and weak spot identification are verified

---

### Story 4.2: Implement NextNoteStrategy Protocol and AdaptiveNoteStrategy

As a **musician using Peach**,
I want the app to intelligently choose which comparisons to present,
So that every comparison maximally improves my pitch discrimination.

**Acceptance Criteria:**

**Given** a NextNoteStrategy protocol
**When** it is defined
**Then** it exposes a method that takes the PerceptualProfile and current settings and returns a Comparison (note1, note2, centDifference)

**Given** an AdaptiveNoteStrategy (implementing NextNoteStrategy)
**When** the user answers correctly
**Then** the next comparison at that note uses a narrower cent difference (harder)

**Given** an AdaptiveNoteStrategy
**When** the user answers incorrectly
**Then** the next comparison at that note uses a wider cent difference (easier)

**Given** an AdaptiveNoteStrategy
**When** selecting the next comparison
**Then** it balances between training nearby the current pitch region and jumping to weak spots, controlled by a tunable ratio (Natural vs. Mechanical)

**Given** a new user with no training history
**When** comparisons are generated
**Then** they use random note selection at 100 cents (1 semitone) with all notes treated as weak (cold start)

**Given** the AdaptiveNoteStrategy
**When** cent differences are computed
**Then** fractional cent precision (0.1 cent resolution) is supported with a practical floor of approximately 1 cent

**Given** the AdaptiveNoteStrategy
**When** unit tests are run
**Then** difficulty adjustment, weak spot targeting, cold start behavior, and Natural/Mechanical balance are verified

---

### Story 4.3: Integrate Adaptive Algorithm into TrainingSession

As a **musician using Peach**,
I want the training loop to use the adaptive algorithm instead of random comparisons,
So that my training is personalized and my profile persists across sessions.

**Acceptance Criteria:**

**Given** the TrainingSession from Epic 3
**When** it is updated
**Then** it uses AdaptiveNoteStrategy (via NextNoteStrategy protocol) instead of the temporary random placeholder

**Given** the app is launched with existing training data
**When** training starts
**Then** the PerceptualProfile is loaded from stored data and the algorithm continues from the user's last known state

**Given** a training session
**When** comparisons are answered
**Then** the PerceptualProfile is updated incrementally after each answer
**And** the next comparison reflects the updated profile

**Given** algorithm parameters
**When** development/testing is in progress
**Then** all algorithm parameters are exposed and adjustable for tuning and discovery

**Given** the integration
**When** unit tests are run
**Then** end-to-end flow from profile loading ‚Üí comparison selection ‚Üí answer recording ‚Üí profile update is verified with mocks

---

Epic 4 complete with 3 stories covering FR9, FR10, FR11, FR12, FR13, FR14, FR15. Does this capture the adaptive algorithm requirements correctly? Any adjustments before we move to Epic 5?

### üßë Human (14:19:20)

yes

### ü§ñ Assistant (14:19:48)

## Epic 5: See Your Progress ‚Äî Profile & Statistics

**Goal:** Users can view their perceptual profile as a visualization, see summary statistics with trends, and access a profile preview from the Start Screen.

**FRs covered:** FR21, FR22, FR23, FR24, FR25, FR26
**Relevant NFRs:** NFR5 (profile rendering < 1 second)
**Relevant UX:** Piano keyboard + confidence band overlay (Canvas + Swift Charts AreaMark), Profile Preview as tappable miniature on Start Screen, empty/cold-start states, summary statistics with trend indicator, system semantic colors, VoiceOver accessibility labels for custom components

---

### Story 5.1: Profile Screen with Perceptual Profile Visualization

As a **musician using Peach**,
I want to see my pitch discrimination ability visualized as a confidence band over a piano keyboard,
So that I can understand where my hearing is strong and where it needs work.

**Acceptance Criteria:**

**Given** the Profile Screen
**When** it is displayed with training data
**Then** it shows a piano keyboard along the X-axis spanning the training range with note names at octave boundaries (C2, C3, C4, etc.)
**And** a confidence band (filled area chart) overlaid above the keyboard showing detection thresholds per note
**And** the band's width represents uncertainty ‚Äî wider where data is sparse, narrower where many comparisons exist
**And** the Y-axis is inverted so improvement (smaller cent differences) moves the band downward toward the keyboard
**And** it uses system semantic colors (system blue/tint for band fill, opacity for confidence range)
**And** it renders within 1 second including computation

**Given** the Profile Screen
**When** there is no training data (cold start)
**Then** the piano keyboard renders fully
**And** the confidence band is absent or shown as a faint uniform placeholder at the 100-cent level
**And** text "Start training to build your profile" appears centered above the keyboard

**Given** the Profile Screen with sparse data
**When** only some notes have been trained
**Then** the confidence band renders where data exists and fades out where it doesn't
**And** no interpolation across large data gaps

**Given** the profile visualization
**When** VoiceOver is active
**Then** it provides an aggregate summary: "Perceptual profile showing detection thresholds from [lowest note] to [highest note]. Average threshold: [X] cents."

**Given** the profile visualization
**When** rendered in dark mode
**Then** it uses system semantic colors and maintains sufficient contrast

---

### Story 5.2: Summary Statistics with Trend Indicator

As a **musician using Peach**,
I want to see my mean detection threshold, standard deviation, and whether I'm improving,
So that I have factual confirmation that training is working.

**Acceptance Criteria:**

**Given** the Profile Screen with training data
**When** summary statistics are displayed
**Then** they show the arithmetic mean of detectable cent differences over the current training range
**And** the standard deviation of detectable cent differences
**And** a trend indicator (improving/stable/declining) presented as an understated directional signal

**Given** the Profile Screen with no training data (cold start)
**When** summary statistics are displayed
**Then** dashes or "‚Äî" appear instead of numbers
**And** the trend indicator is hidden

**Given** the PerceptualProfile
**When** summary statistics are computed
**Then** they are derived from stored per-answer data (FR26)

---

### Story 5.3: Profile Preview on Start Screen and Navigation

As a **musician using Peach**,
I want to see a miniature of my pitch profile on the Start Screen that I can tap to see details,
So that I can glance at my progress without navigating away.

**Acceptance Criteria:**

**Given** the Start Screen
**When** it is displayed
**Then** a Profile Preview is shown ‚Äî a compact, simplified version of the full profile visualization (same confidence band shape, no axis labels, no note names, no numerical values)
**And** it is sized appropriately as a secondary element (~full width, ~60-80pt tall)

**Given** the Profile Preview
**When** the user taps it
**Then** it navigates to the full Profile Screen

**Given** the Profile Preview with no training data
**When** displayed on the Start Screen
**Then** it shows a placeholder shape that looks intentional, not broken

**Given** the Profile Preview
**When** VoiceOver is active
**Then** it announces "Your pitch profile. Tap to view details." (or with threshold data if available)

**Given** the Profile Screen
**When** dismissed (back navigation or swipe)
**Then** the user returns to the Start Screen

**Given** the Profile Preview rendering
**When** compared to the full visualization
**Then** it shares the same rendering logic, scaled down and stripped of labels (single implementation)

---

Epic 5 complete with 3 stories covering FR21, FR22, FR23, FR24, FR25, FR26 and NFR5. Does this capture the profile and statistics requirements correctly? Any adjustments before we move to Epic 6?

### üßë Human (14:22:59)

yes

### ü§ñ Assistant (14:23:15)

## Epic 6: Make It Yours ‚Äî Settings & Configuration

**Goal:** Users can customize the training experience: algorithm behavior, note range, duration, reference pitch, and sound source selection.

**FRs covered:** FR30, FR31, FR32, FR33, FR34, FR35, FR36
**Relevant Architecture:** @AppStorage (UserDefaults) for all settings, TrainingSession reads settings when requesting next comparison, passes duration to NotePlayer
**Relevant UX:** Stock SwiftUI Form with Slider, Stepper, Picker controls, auto-save (no save button), changes take effect on next comparison, standard Form section grouping

---

### Story 6.1: Settings Screen with All Configuration Options

As a **musician using Peach**,
I want to customize algorithm behavior, note range, note duration, reference pitch, and sound source,
So that the training experience matches my preferences and musical context.

**Acceptance Criteria:**

**Given** the Settings Screen
**When** it is displayed
**Then** it shows the following controls in a stock SwiftUI Form with logical section grouping:
- Natural vs. Mechanical slider (`Slider`)
- Note range lower bound (`Picker` or `Stepper`)
- Note range upper bound (`Picker` or `Stepper`)
- Note duration (`Stepper` or `Slider`)
- Reference pitch (`Stepper`, default A4 = 440Hz)
- Sound source selection (`Picker`, MVP: sine wave only)

**Given** the user changes any setting
**When** the value is adjusted
**Then** it is persisted immediately via `@AppStorage` ‚Äî no save/cancel buttons

**Given** settings controls
**When** interacted with
**Then** all controls are bounded (sliders have min/max, steppers have ranges) ‚Äî no form validation needed

**Given** the Settings Screen
**When** dismissed (back navigation or swipe)
**Then** the user returns to the Start Screen

**Given** the Settings Screen
**When** accessed from the Training Screen
**Then** training stops, the Settings Screen is shown, and dismissal returns to the Start Screen

---

### Story 6.2: Apply Settings to Training in Real Time

As a **musician using Peach**,
I want my setting changes to take effect immediately on the next comparison,
So that I can feel the difference and find my preferred configuration.

**Acceptance Criteria:**

**Given** the user has changed the Natural vs. Mechanical slider
**When** the next comparison is selected
**Then** the AdaptiveNoteStrategy uses the updated balance ratio

**Given** the user has changed the note range bounds
**When** the next comparison is selected
**Then** the AdaptiveNoteStrategy only selects notes within the new range

**Given** the user has changed the note duration
**When** the next note is played
**Then** the NotePlayer uses the updated duration

**Given** the user has changed the reference pitch
**When** the next note is played
**Then** frequencies are derived from the new reference pitch

**Given** settings are persisted
**When** the app is restarted
**Then** all settings retain their last configured values
**And** training uses the persisted settings

**Given** the settings integration
**When** unit tests are run
**Then** the flow from settings change ‚Üí TrainingSession reading updated values ‚Üí effect on next comparison/note is verified

---

Epic 6 complete with 2 stories covering FR30, FR31, FR32, FR33, FR34, FR35, FR36. Does this capture the settings requirements correctly? Any adjustments before we move to Epic 7?

### üßë Human (14:24:30)

yes

### ü§ñ Assistant (14:24:59)

## Epic 7: Polish & Ship ‚Äî Platform, Localization & Info

**Goal:** Users can use the app in English or German, on iPhone and iPad, in both orientations, with accessibility support and an Info Screen.

**FRs covered:** FR37, FR38, FR39, FR40, FR41, FR43
**Relevant NFRs:** NFR6 (VoiceOver labels), NFR7 (color contrast), NFR8 (44x44pt tap targets)
**Relevant Architecture:** String Catalogs (Localizable.xcstrings) for English/German
**Relevant UX:** Stock SwiftUI handles Dynamic Type, dark mode, Reduce Motion automatically; custom components need explicit accessibility labels; Info Screen as .sheet() with app name, developer, copyright, version; portrait primary, landscape supported; iPhone primary, iPad supported

---

### Story 7.1: English and German Localization

As a **musician using Peach**,
I want to use the app in English or German,
So that the interface is in my preferred language.

**Acceptance Criteria:**

**Given** all user-facing strings in the app
**When** localization is applied
**Then** every string is externalized to String Catalogs (Localizable.xcstrings)
**And** English and German translations are provided for all strings

**Given** the device language is set to German
**When** the app is launched
**Then** all UI text appears in German

**Given** the device language is set to English (or any unsupported language)
**When** the app is launched
**Then** all UI text appears in English (default)

**Given** custom components (profile visualization, profile preview, feedback indicator)
**When** they display text (e.g., "Start training to build your profile", statistics labels)
**Then** that text is also localized

---

### Story 7.2: Accessibility Audit and Custom Component Labels

As a **musician using Peach**,
I want all screens to be fully accessible with VoiceOver, Dynamic Type, and sufficient contrast,
So that the app is usable with assistive technology.

**Acceptance Criteria:**

**Given** all stock SwiftUI components
**When** VoiceOver is active
**Then** they are automatically labeled and navigable (no additional work needed)

**Given** custom components (profile visualization, profile preview, feedback indicator)
**When** VoiceOver is active
**Then** the profile visualization announces an aggregate summary of detection thresholds
**And** the profile preview announces "Your pitch profile. Tap to view details." (with threshold data if available)
**And** the feedback indicator announces "Correct" or "Incorrect"

**Given** all text in the app
**When** Dynamic Type is set to the largest accessibility size
**Then** text scales correctly and layout does not break

**Given** all UI elements
**When** tested for color contrast
**Then** system semantic colors provide sufficient contrast in both light and dark mode

**Given** the Training Screen
**When** tested with eyes closed
**Then** the audio-haptic loop works without visual feedback ‚Äî a complete training session can be performed

**Given** system settings
**When** Reduce Motion is enabled
**Then** any transitions (feedback indicator appearance/disappearance) respect the setting

---

### Story 7.3: iPhone, iPad, Portrait, and Landscape Support

As a **musician using Peach**,
I want to use the app on my iPhone or iPad in any orientation,
So that training works on whatever device I have at hand.

**Acceptance Criteria:**

**Given** the app running on iPhone
**When** displayed in portrait
**Then** all screens render correctly with Training Screen buttons optimized for one-handed thumb reach

**Given** the app running on iPhone
**When** rotated to landscape
**Then** all screens adapt via SwiftUI automatic layout
**And** Training Screen buttons reflow to a horizontal arrangement

**Given** the app running on iPad
**When** displayed in any orientation
**Then** layouts scale naturally ‚Äî no iPad-specific layouts or split views

**Given** the app running on iPad
**When** used in windowed/compact mode
**Then** layouts compress gracefully, the same way they do on smaller iPhones

**Given** all screens
**When** tested on iPhone SE (smallest) and iPhone 16 Pro Max (largest)
**Then** layouts are functional and visually appropriate on both extremes

---

### Story 7.4: Info Screen

As a **musician using Peach**,
I want to view basic information about the app,
So that I know the version and who made it.

**Acceptance Criteria:**

**Given** the Start Screen
**When** the user taps the Info button (`info.circle` SF Symbol)
**Then** an Info Screen is presented as a `.sheet()`

**Given** the Info Screen
**When** displayed
**Then** it shows the app name (Peach), developer name, copyright notice, and app version number (pulled from bundle)
**And** it uses stock SwiftUI layout with minimal content

**Given** the Info Screen
**When** dismissed (swipe down or tap dismiss)
**Then** the user returns to the Start Screen

**Given** the Info Screen text
**When** localization is active
**Then** static labels are localized (app name and developer name remain as-is)

---

Epic 7 complete with 4 stories covering FR37, FR38, FR39, FR40, FR41, FR43 and NFR6, NFR7, NFR8. Does this capture the polish and ship requirements correctly? Any adjustments?

All 7 epics are now complete with a total of **18 stories**. Ready to finalize?

**Select an Option:** [A] Advanced Elicitation [P] Party Mode [C] Continue

### üßë Human (14:27:04)

c

### ü§ñ Assistant (14:27:08)

Let me save all epics and stories to the document now.

### ü§ñ Assistant (14:29:21)

All 7 epics and 18 stories saved to the document. Now loading the final validation step.

### ü§ñ Assistant (14:29:47)

Let me now run through all validation checks.

---

### 1. FR Coverage Validation

Checking every FR against the stories:

| FR | Story | Covered? |
|---|---|---|
| FR1 | 3.1 (Start Training button) | Yes |
| FR2 | 3.2 (two sequential notes) | Yes |
| FR3 | 3.2, 3.3 (Higher/Lower answer) | Yes |
| FR4 | 3.3 (Feedback Indicator) | Yes |
| FR5 | 3.3 (haptic on incorrect) | Yes |
| FR6 | 3.4 (stop via navigation) | Yes |
| FR7 | 3.4 (discard incomplete) | Yes |
| FR7a | 3.4 (foreground returns to Start) | Yes |
| FR8 | 3.3 (button disable/enable) | Yes |
| FR9 | 4.2 (profile-based selection) | Yes |
| FR10 | 4.2 (difficulty adjustment) | Yes |
| FR11 | 4.2 (Natural/Mechanical balance) | Yes |
| FR12 | 4.2 (cold start 100 cents) | Yes |
| FR13 | 4.1, 4.3 (profile persistence) | Yes |
| FR14 | 4.2 (fractional cent precision) | Yes |
| FR15 | 4.3 (expose parameters) | Yes |
| FR16 | 2.1 (sine wave generation) | Yes |
| FR17 | 2.1 (attack/release envelopes) | Yes |
| FR18 | 2.1 (same timbre) | Yes |
| FR19 | 2.2 (configurable duration) | Yes |
| FR20 | 2.2 (configurable reference pitch) | Yes |
| FR21 | 5.1 (keyboard + confidence band) | Yes |
| FR22 | 5.3 (Profile Preview) | Yes |
| FR23 | 5.3 (navigate to Profile Screen) | Yes |
| FR24 | 5.2 (mean + std dev) | Yes |
| FR25 | 5.2 (trend indicator) | Yes |
| FR26 | 5.2 (compute from stored data) | Yes |
| FR27 | 1.2 (store comparison records) | Yes |
| FR28 | 1.2 (local persistence) | Yes |
| FR29 | 1.2 (data integrity) | Yes |
| FR30 | 6.1 (Natural/Mechanical slider) | Yes |
| FR31 | 6.1 (note range) | Yes |
| FR32 | 6.1 (note duration) | Yes |
| FR33 | 6.1 (reference pitch) | Yes |
| FR34 | 6.1 (sound source) | Yes |
| FR35 | 6.1 (settings persistence) | Yes |
| FR36 | 6.2 (immediate application) | Yes |
| FR37 | 7.1 (English + German) | Yes |
| FR38 | 7.2 (accessibility) | Yes |
| FR39 | 7.3 (iPhone + iPad) | Yes |
| FR40 | 7.3 (portrait + landscape) | Yes |
| FR41 | 7.3 (iPad windowed) | Yes |
| FR42 | 3.3 (one-handed, large targets) | Yes |
| FR43 | 7.4 (Info Screen) | Yes |

**All 44 FRs covered.**

### 2. Architecture Implementation Validation

- **Starter template:** Epic 1, Story 1.1 creates Xcode project from template ‚Äî correct.
- **Database/entity creation:** ComparisonRecord created in Story 1.2 (first story that needs it) ‚Äî correct. No upfront creation of unused models.

### 3. Story Quality Validation

- All 18 stories have clear acceptance criteria in Given/When/Then format
- All stories are sized for single dev agent completion
- All stories reference specific FRs they implement
- No forward dependencies detected

### 4. Epic Structure Validation

- All epics deliver user value (not technical milestones)
- Epic 1 is the only "developer-facing" epic, justified as the necessary foundation
- Dependencies flow naturally: 1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí 5/6/7

### 5. Dependency Validation

**Epic independence:**
- Epic 1: Standalone (data foundation)
- Epic 2: Standalone (audio, uses no other epics)
- Epic 3: Uses Epic 1 (data) + Epic 2 (audio) ‚Äî standalone training loop with random comparisons
- Epic 4: Uses Epic 1 (data) + replaces placeholder in Epic 3 ‚Äî standalone algorithm logic
- Epic 5: Uses Epic 1 (data) + Epic 4 (profile) ‚Äî standalone visualization
- Epic 6: Uses Epic 3 + 4 (settings affect training) ‚Äî standalone settings
- Epic 7: Cross-cutting polish ‚Äî standalone

**Within-epic story dependencies:** All stories within each epic build only on previous stories. No forward dependencies found.

---

**All validations pass.** The document is complete with 7 epics and 18 stories covering all 44 functional requirements.

**All validations complete!** [C] Complete Workflow

### üßë Human (14:30:58)

c

### ü§ñ Assistant (14:31:18)

**Epics and Stories workflow complete!**

Your `docs/planning-artifacts/epics.md` is finalized with **7 epics and 18 stories** covering all 44 functional requirements.

---

Here's what comes next in the BMM workflow. Each should be run in a **fresh context window**:

**Required ‚Äî Next Step:**

- **Check Implementation Readiness** ‚Äî `/bmad-bmm-check-implementation-readiness`
  Agent: Winston (Architect)
  Ensures PRD, UX, Architecture, and Epics/Stories are aligned and ready for implementation.

**Then ‚Äî Implementation Phase:**

- **Sprint Planning** ‚Äî `/bmad-bmm-sprint-planning`
  Agent: Bob (Scrum Master)
  Generates a sprint plan that kicks off implementation by producing the sequence for development.

---

Any questions about the epics and stories?

### üßë Human (14:32:44)

<command-name>/exit</command-name>
            <command-message>exit</command-message>
            <command-args></command-args>

### üßë Human (14:32:44)

<local-command-stdout>See ya!</local-command-stdout>

## Session 2 (`383714ff‚Ä¶`)

*2026-02-12 14:33:12 UTC ‚Üí 2026-02-12 14:35:34 UTC*

### üßë Human (14:33:12)

<command-message>bmad-bmm-check-implementation-readiness</command-message>
<command-name>/bmad-bmm-check-implementation-readiness</command-name>

### ü§ñ Assistant (14:33:21)

Now let me load and follow the first step.

### ü§ñ Assistant (14:33:34)

Beginning **Document Discovery** to inventory all project files.

I will:

1. Search for all required documents (PRD, Architecture, Epics, UX)
2. Group sharded documents together
3. Identify any duplicates (whole + sharded versions)
4. Present findings for your confirmation

Let me search for all document types now.

### üßë Human (14:34:39)

[Request interrupted by user for tool use]

### üßë Human (14:35:02)

Before we proceed, commit all changes with a meaningful message to git.

### ü§ñ Assistant (14:35:05)

Let me check the current state of things first.

### ü§ñ Assistant (14:35:34)

Committed as `a14387c`. The commit includes:

- **New file:** `docs/planning-artifacts/epics.md` ‚Äî the epics & stories document
- **Modified:** `docs/planning-artifacts/architecture.md` ‚Äî removed the redundant `note1CentOffset` field from the comparison record model

Working tree is now clean. Want me to resume the implementation readiness check?

---

[‚Üê Previous (df955d28ff)](df955d28ff.md) | [Index](index.md) | [Next (0a89329199) ‚Üí](0a89329199.md)
