---
commit: "7c1eb9b60ec931bf420e583cfb8d34578cfe30a1"
date: "2026-02-11T17:48:38+01:00"
author: "Michael Sch√ºrig <michael@schuerig.de>"
subject: "Document core requirements for Peach pitch training app"
sessions: ["eab1532b-ae56-4200-aa8d-369150dae5dd"]
prev: "7bb2a2e9e8.md"
next: "1706776b0c.md"
---

[‚Üê Previous (7bb2a2e9e8)](7bb2a2e9e8.md) | [Index](index.md) | [Next (1706776b0c) ‚Üí](1706776b0c.md)

# Commit 7c1eb9b60e

**Date:** 2026-02-11 17:48:38 UTC+01:00  
**Author:** Michael Sch√ºrig <michael@schuerig.de>

## Commit message

Document core requirements for Peach pitch training app

Brainstorming session covering the full requirements for an iOS app
that trains musicians' pitch discrimination through adaptive,
weakness-targeted exercises rather than score-based testing.

Co-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>

## Changed files

```
.../brainstorming-session-2026-02-11.md            | 162 ++++++++++++++++++++-
 1 file changed, 155 insertions(+), 7 deletions(-)
```

## Session `eab1532b‚Ä¶`

*2026-02-11 13:41:34 UTC ‚Üí 2026-02-11 16:48:40 UTC*

### üßë Human (13:41:34)

<command-message>bmad-brainstorming</command-message>
<command-name>/bmad-brainstorming</command-name>

### ü§ñ Assistant (13:41:44)

Let me check if there's an existing session document for today.

### ü§ñ Assistant (13:41:47)

No existing session found. Let me initialize a fresh one.

### ü§ñ Assistant (13:48:33)

Welcome Michael! I'm excited to facilitate your brainstorming session. I'll guide you through proven creativity techniques to generate innovative ideas and breakthrough solutions.

**Let's set up your session for maximum creativity and productivity:**

**Session Discovery Questions:**

1. **What are we brainstorming about?** (The central topic or challenge)
2. **What specific outcomes are you hoping for?** (Types of ideas, solutions, or insights)

### üßë Human (13:50:14)

I want to find and document the core requirements for an iOS app for musicians that helps with hearing minute pitch differences.

### ü§ñ Assistant (13:50:20)

Great topic, Michael! Let me make sure I've captured this clearly.

**Session Analysis:**

Based on your response, I understand we're focusing on **defining the core requirements for an iOS app that helps musicians train their ability to hear and distinguish minute pitch differences** ‚Äî essentially fine-grained ear training beyond standard semitone recognition.

**Session Parameters:**

- **Topic Focus:** Core requirements discovery for a pitch discrimination training app for iOS musicians
- **Primary Goals:** Document a clear set of core requirements ‚Äî the essential features, interactions, and design considerations that make this app effective and distinct

**Does this accurately capture what you want to achieve?**

### üßë Human (13:54:17)

The app is not targetted at people making music on iOS, but obviously to musicians who own an iPhone or iPad. For a lot of musicians, such as singers, string, woodwind and brass players, intonation is a challenge. This challenge starts with precisely hearing and distinguishing pitches. The app is intended to help with training this ability.

### ü§ñ Assistant (13:54:23)

Thank you for the clarification, that's an important distinction!

**Session Analysis (Revised):**

We're focusing on **core requirements for an iOS app that trains musicians' ability to precisely hear and distinguish pitches**. The target audience is instrumentalists and vocalists ‚Äî singers, string players, woodwind and brass players ‚Äî for whom intonation is a constant practical challenge. The app addresses the foundational skill underneath good intonation: the ear itself. It's a training tool for pitch discrimination, not a performance or production tool.

**Session Parameters:**

- **Topic Focus:** Core requirements for an iOS pitch discrimination ear training app
- **Primary Goals:** Discover and document the essential requirements ‚Äî what the app must do, who it serves, and what makes it effective for training fine pitch perception
- **Target Users:** Musicians (singers, string, woodwind, brass) who struggle with or want to sharpen intonation skills

**Does this accurately capture what you want to achieve?**

### üßë Human (13:54:53)

yes

### ü§ñ Assistant (13:54:56)

Excellent! Session setup is locked in. Let me save this and get you set up for the next phase.

### ü§ñ Assistant (13:56:08)

**Session setup complete!** I have a clear understanding of your goals and can select the perfect techniques for your brainstorming needs.

**Ready to explore technique approaches?**

1. **User-Selected Techniques** ‚Äî Browse our complete technique library and pick what appeals to you
2. **AI-Recommended Techniques** ‚Äî Get customized suggestions based on your goals
3. **Random Technique Selection** ‚Äî Discover unexpected creative methods
4. **Progressive Technique Flow** ‚Äî Start broad, then systematically narrow focus

Which approach appeals to you most? (Enter 1-4)

### üßë Human (13:57:51)

2

### ü§ñ Assistant (13:58:27)

Great choice! Let me analyze your session context and recommend the perfect brainstorming techniques.

**Analyzing Your Session Goals:**

- **Topic:** Core requirements for an iOS pitch discrimination ear training app for musicians
- **Goals:** Discover and document essential requirements ‚Äî features, interactions, and design considerations
- **Session Type:** Requirements discovery for a focused, well-defined product concept

**Loading Brain Techniques Library for AI Analysis... Done.**

---

**My AI Analysis Results:**

Your session is a **requirements discovery** task for a specific product ‚Äî it benefits from structured exploration to cover all angles, combined with perspective-shifting to ensure you don't miss non-obvious requirements. Here's my recommended sequence:

---

**Phase 1: Foundation Setting**
**Question Storming** from Deep (Energy: Medium)

- **Why this fits:** Before jumping to "the app should do X," we generate *all the right questions* first ‚Äî what do musicians actually struggle with? What does "minute pitch difference" mean in practice? What makes training stick? This ensures we solve the right problem.
- **Expected outcome:** A rich map of questions that define the full problem space ‚Äî user needs, technical constraints, pedagogical considerations, edge cases.
- **Sample facilitation:** I'll prompt you with angles like "What questions would a beginner singer ask?" or "What would a professional cellist need to know?"
- **Your role:** Generate as many questions as possible without filtering. No answers yet ‚Äî just questions.

**Phase 2: Idea Generation**
**Role Playing** from Collaborative (Energy: Medium-High)

- **Why this builds on Phase 1:** Now we step into the shoes of different users ‚Äî a choir singer, a violin student, a brass player warming up, a teacher assigning practice ‚Äî and generate requirements from each perspective. The questions from Phase 1 become fuel.
- **Expected outcome:** Requirements grounded in real user scenarios, covering diverse musician types and use cases.
- **Sample facilitation:** "You're a gigging jazz trumpet player. You have 10 minutes before a set. What do you need from this app?"
- **Your role:** Embody each persona and think from their point of view.

**Phase 3: Refinement & Deepening**
**Morphological Analysis** from Deep (Energy: Medium)

- **Why this concludes effectively:** We take the raw requirements and systematically map them across key dimensions ‚Äî training modes, feedback mechanisms, difficulty progression, sound design, UX patterns ‚Äî to ensure completeness and spot gaps.
- **Expected outcome:** A structured requirements map that's both comprehensive and organized, ready to become a requirements document.
- **Sample facilitation:** We'll build a grid of parameters (e.g., "exercise type" x "feedback style" x "difficulty model") and explore combinations.
- **Your role:** Evaluate combinations, flag what's essential vs. nice-to-have.

---

**Total Estimated Time:** 45-60 minutes
**Session Focus:** Comprehensive requirements discovery through questioning, perspective-taking, and systematic analysis

---

**Does this approach sound right for your session?**

- **[C] Continue** ‚Äî Begin with these recommended techniques
- **[Modify]** ‚Äî I'd like to adjust the technique selection
- **[Details]** ‚Äî Tell me more about any specific technique
- **[Back]** ‚Äî Return to approach selection

### üßë Human (13:59:52)

C

### ü§ñ Assistant (14:01:08)

**Outstanding! Let's begin our first technique with true collaborative facilitation.**

I'm excited to facilitate **Question Storming** with you as a creative partner. The rule is simple and powerful: **we generate only questions ‚Äî no answers yet.** Every question opens a door we might not have noticed. The goal is to map the entire problem space before we start solving anything.

**Technique: Question Storming**
**Focus:** Uncover the full landscape of what this app needs to address
**Energy:** Reflective, curious, exploratory

**My Coaching Approach:**
- I'll introduce angles one at a time
- We'll explore each together through back-and-forth dialogue
- No question is too basic, too wild, or too specific
- You can always say "let's explore this more" or "next technique" to move on

---

**Let's start with the most fundamental angle: The User's Problem.**

Before we think about features or screens, let's ask questions about what musicians actually experience when they struggle with pitch. Here are a few to get us going:

- What exactly happens in a musician's mind when they can't tell if a note is sharp or flat?
- At what point in a musician's development does pitch discrimination become a bottleneck?
- Is the challenge different for someone who plays a fixed-pitch instrument (piano) versus a continuous-pitch instrument (violin)?

Now it's your turn, Michael ‚Äî what questions come to mind about the core problem this app is trying to solve? Don't filter ‚Äî just let the questions flow.

### üßë Human (14:07:28)

I need to establish a few things from the outset. This is not meant to become a commercial product. This project has a couple of goals: I, personally, want to improve my pitch hearing and intonation. I am a software developer and I want to learn about developing for iOS. I want to learn about AI-assisted software development.

I already have experience with another pitch training app, called InTune. That app is a nice start, but after some time with it I can say that it falls short on how I would like to train this skill. Let's elaborate on that.

### ü§ñ Assistant (14:07:40)

This is excellent foundational context, Michael. Let me make sure I've captured this clearly:

**Project Context (Revised):**
- **Personal project**, not commercial ‚Äî built for your own use
- **Three learning goals:** improve your own pitch perception, learn iOS development, explore AI-assisted development
- **You're the primary user** ‚Äî a software developer who is also a musician
- **Prior experience:** You've used InTune and found it lacking after sustained use

This changes the lens significantly. We're not designing for an abstract market ‚Äî we're designing for *you*, informed by real frustrations with a real app you've used.

---

**This is a perfect Question Storming thread. Let's dig into InTune's shortcomings.**

Rather than me guessing, I want to hear from you directly: where does InTune fall short? What's the gap between what it offers and how you actually want to train?

Some angles to get your thinking going:

- What was the moment you first thought "this isn't enough"?
- What does a typical InTune session look like for you, and where does it feel unsatisfying?
- Is it about *what* it trains, *how* it trains, or *how it responds to your progress* ‚Äî or all three?

Tell me about your experience. What's missing?

### üßë Human (14:16:35)

InTune (and a lot of other training apps for other things) follow a paradigm that I like to call "training through testing". They present the user with tasks of ever increasing difficulty until they fail, or fail for the third time. This is the point these apps consider the users level of ability.

In contrast, I'd like the training schedule to be focussed on the users weak spots. I'm not interested in an overall score. In the case of hearing pitch differences, not every note along the musically useful range is the same. A 5 cent difference, say, may be easily detectable in one register and currently unnoticeable in another. This will differ from person to person.

### ü§ñ Assistant (14:16:53)

This is a *really* sharp distinction, Michael. Let me make sure I've got the core insight:

**InTune's paradigm: "Training through testing"**
- Linear difficulty escalation until failure
- Produces an overall score/level
- Treats pitch discrimination as a single uniform ability

**Your paradigm: Targeted weakness training**
- Not interested in a score ‚Äî interested in *actually getting better*
- Pitch discrimination is **not uniform** across the frequency range
- A 5-cent difference at 200Hz and a 5-cent difference at 2000Hz are completely different perceptual challenges
- Each person has a unique "map" of strong and weak spots across pitch and register
- Training should find and focus on where *you* are weakest

This is a fundamental architectural difference. The app needs to build a **perceptual profile** of the user ‚Äî not a single number, but a landscape.

---

Let me stay in Question Storming mode and dig deeper into this, because there's a lot to unpack:

- Beyond register (high vs. low), are there other dimensions where pitch discrimination varies? For example, does timbre matter ‚Äî is it easier to hear pitch differences on a pure sine wave versus a complex instrument tone?
- When you say "5 cents" ‚Äî is cents the right unit for how you think about this? Is there a threshold granularity that matters to you musically?
- How would you *want* to experience your weak spots? Would you want to see them visualized? Or is it more about the app silently steering you toward them during practice?
- Does the direction matter ‚Äî is detecting "sharp" different from detecting "flat" for you?

What resonates here? And what other dimensions of this "perceptual map" are you thinking about?

### üßë Human (14:32:08)

Cents is exactly the right unit for differences in pitch. A semitone is always subdivided into 100 cents. Important: for musical purposes we're not thinking in frequencies, but in musical notes. These notes are anchored to frequencies, in that the reference pitch A4 is usually 440 Hz (it can be different, but let's go with 440). You may want to look up how human pitch perception works, especially regarding octaves: An octave is perceived as essentially the same note at a higher pitch. This is not just an arbitrary fact of human physiology, but there's a physical reason behind it, because a tone and its octave share a lot of harmonics.

A visual indication of accuracy over frequency would be nice.

Direction (probably) does not matter and for the start I'd like to ignore it. Let's focus just on distance.

In contrast to InTune and other training games, I don't think this app needs a concept of "games" or "trials" or "attempts". The user can end a training session whenever they like and continue at the same point at a later time.

### ü§ñ Assistant (14:32:24)

Excellent. This is sharpening the picture significantly. Let me capture the key requirements emerging:

**Domain Model:**
- **Cents** as the unit of pitch difference (100 cents = 1 semitone)
- **Musical notes**, not frequencies, as the frame of reference
- A4 = 440 Hz as the reference pitch
- Octave equivalence is perceptually real and physically grounded (shared harmonics)
- The app thinks in terms of the musical pitch space, not raw Hz

**Interaction Model:**
- No games, trials, or attempts ‚Äî no gamification structure
- **Continuous training** ‚Äî pick up where you left off, stop when you want
- No session boundaries imposed by the app
- Focus on **distance** (how many cents apart), not direction (sharp vs. flat) ‚Äî at least initially

**Feedback Model:**
- Visual indication of accuracy mapped across the frequency/note range
- A kind of "heat map" of where the user is strong and weak

---

This is great territory for more questions. The "no sessions, just continuous training" idea is deceptively simple ‚Äî it has real implications:

- What does the core interaction loop look like? The app plays two tones and you say "same or different"? Or is it "which is higher"? Or something else entirely?
- How does the app decide *what to present next*? If there are no levels or rounds, it needs some internal logic for selecting the next exercise. Is this where the weakness-targeting comes in ‚Äî it always gravitates toward your frontier?
- When you say "continue at the same point" ‚Äî does the app need to persist your perceptual profile across sessions? So it's building a long-term model of *your* hearing?
- What's the minimum viable interaction? What's the simplest thing the app could do that would already be more useful to you than InTune?

What does the core exercise actually look like in your head, Michael?

### üßë Human (14:41:03)

The essential flow is this: The app plays a pair of notes in sequence, without overlap. The user needs to answer if the second note is higher or lower. The tricky question for this app is how to choose the next pair of notes, based on the user's history and whether the answer was correct.

### ü§ñ Assistant (14:41:18)

Now we're at the heart of it. So the core loop is:

**Core Interaction:**
1. App plays Note A, then Note B (sequential, no overlap)
2. User answers: **higher or lower**
3. App records the result and selects the next pair

And the *entire intelligence* of this app lives in step 3 ‚Äî **how it chooses the next pair.** That's the algorithm that replaces the "training through testing" paradigm.

Let me capture what we already know about that selection logic:

- It should target the user's **weak areas** ‚Äî notes/registers where they struggle
- It needs a **persistent model** of the user's discrimination ability across the pitch range
- The model is at minimum two-dimensional: **which note** (position in the range) and **how far apart** (cents)
- It should adapt over time as the user improves

---

This is the most important question in the entire project, so let's really dig in:

- When the user gets an answer **right**, what should happen? Make it harder in the same region (fewer cents)? Move to a weaker region? Some mix?
- When the user gets it **wrong**, what should happen? Repeat something similar? Widen the gap and try again? Stay in that region longer?
- Should the algorithm feel like a **conversation** ‚Äî responsive and natural ‚Äî or is it fine if it feels mechanical as long as it's effective?
- Is there a floor? For example, if two notes are 1 cent apart, that's essentially imperceptible for anyone. What's the smallest meaningful difference you'd want to train toward?
- And what about the **range of notes** ‚Äî are we talking the full piano range? Or the range relevant to your instrument? What do you play?

What's your instinct on the adaptive logic? How should it *feel* when you're using it?

### üßë Human (14:58:35)

I think we need to identify a few tunable parameters for the next-note-next-difference algorithm.
Right answer: percentage of making it harder based on a close-by note vs choosing another weak spot.
Wrong answer: percentage of making it easier based on a close-by note vs choosing another weak spot.
A weak spot should be chosen randomly, but weighted by the user's ability.

The app should handle fractional cents (one decimal place), but in practice 1 cent will probably be a lower limit.

Regarding natural vs mechanical: I think the app could offer users a slider for this that behind the scenes affects multiple parameters of the algorithm.

For the range of notes, I can think of multiple options
* let the user decide in the app settings
* start with a narrow range, say an octave up and down around C4, and adaptively grow the range when the user advances below a given threshold at the margins.

### ü§ñ Assistant (14:58:50)

This is getting really concrete. Let me capture these as emerging requirements:

**Next-Pair Algorithm Parameters:**

- **On correct answer:** Tunable ratio between "harder in same region" vs. "jump to another weak spot"
- **On wrong answer:** Tunable ratio between "easier in same region" vs. "jump to another weak spot"
- **Weak spot selection:** Random but weighted by weakness ‚Äî worse areas get picked more often
- **Precision:** Fractional cents to one decimal place, practical floor around 1 cent

**"Natural vs. Mechanical" Slider:**
- A single user-facing control that behind the scenes adjusts multiple algorithm parameters
- This is a really elegant idea ‚Äî it gives the user a feel-based preference without exposing the complexity

**Note Range ‚Äî Two Approaches:**
- Manual: user sets range in settings
- Adaptive: start narrow (octave around C4), expand when performance at the edges crosses a threshold

---

Let me push on a few more dimensions here:

- For the **perceptual profile** the app builds over time ‚Äî how should it handle the fact that ability *decays*? If you haven't trained a region in weeks, should the app gradually "forget" your proficiency there and revisit it?
- The **note pair itself** ‚Äî are both notes always in the same octave? Or could Note A be a C4 and Note B be a slightly sharp C5? Or do we always stay close in register so it's purely about pitch discrimination, not interval recognition?
- **Timbre** ‚Äî pure sine wave? Piano tone? Your own instrument's sound? Does this matter to you at this stage or is it a "maybe later" feature?
- On the adaptive range expansion ‚Äî that's interesting because it means the app has two frontiers: **narrowing the cent gap** and **widening the note range**. Should those be independent or connected?

What's your thinking on these?

### üßë Human (15:08:01)

Perceptual profile: For a start we can leave this to the next-pair algorithm. If ability does decay, the algorithm as specified so far will already react sensibly.

For the time being both notes will be close by. The starting distance should be a semitone.

Timbre: I don't have a suitably licensed library of instrument samples, so for practical reasons the timbre is limited to what can be easily generated. For a start, simple sine tones are enough. But in the future it should be possible to use sampled sounds and so control external instruments/apps via MIDI. Also, for the time being, both notes in a pair are of the same kind.

The focus is definitely on narrowing the cent gap. Widening the note range is only incidental.

### ü§ñ Assistant (15:08:18)

Great, this is becoming very well-defined. Let me capture the refined picture:

**Note Pairs:**
- Both notes close together in register ‚Äî this is pitch discrimination, not interval recognition
- Starting distance: one semitone (100 cents)
- The algorithm narrows the gap over time as the user improves
- Both notes in a pair always use the same timbre

**Timbre:**
- **MVP:** Sine tones (practically achievable, no licensing issues)
- **Future:** Sampled sounds and MIDI output to control external instruments/apps
- Architecture should allow for swappable sound sources

**Primary Training Axis:**
- Narrowing the cent gap is the main goal
- Widening the note range is secondary/incidental

**Decay Handling:**
- No special mechanism needed ‚Äî the adaptive algorithm will naturally resurface weak areas if ability fades

---

I think we've built a strong foundation through Question Storming. Let me take stock of where we are before we continue.

**Requirements Captured So Far:**

1. Personal project ‚Äî you are the primary user
2. Core loop: hear two sequential notes, answer higher/lower
3. Adaptive algorithm selects next pair based on persistent perceptual profile
4. Profile maps ability across note range and cent-gap granularity
5. Tunable parameters for the algorithm's behavior (harder vs. jump to weak spot)
6. User-facing "natural vs. mechanical" slider affecting multiple parameters
7. Note range: configurable or adaptive starting from an octave around C4
8. Fractional cents (1 decimal place), practical floor ~1 cent
9. No gamification ‚Äî continuous training, stop and resume anytime
10. Visual display of accuracy across the note range
11. Sine tones for MVP, future support for samples and MIDI
12. Musical note model (not frequency), A4=440Hz reference
13. Focus on distance, not direction

**Ready for the next technique?** We could shift into **Role Playing** ‚Äî where you step into your own shoes as a user in different scenarios (first launch, mid-session, returning after a week, hitting a plateau) to surface requirements we haven't thought of yet. Or we can keep digging here.

What feels right?

### üßë Human (15:10:36)

Let's shift into Role Playing.

### ü§ñ Assistant (15:10:46)

**Let's transition to Role Playing!**

What we uncovered in Question Storming gives us a strong foundation ‚Äî now let's stress-test it by *living inside the app* from different moments in your journey as a user.

**Technique: Role Playing**
**Focus:** Experience the app from your perspective at different points in time to surface hidden requirements
**Energy:** Immersive, practical, scenario-driven

Since you're the primary user, this is particularly powerful ‚Äî we're not imagining hypothetical personas, we're exploring *your actual future experience* with this app.

---

**Scenario 1: First Launch**

You've just built the app and installed it on your iPhone for the first time. You open it.

- What do you see?
- What do you need to understand before you can start training?
- How much setup are you willing to do before hearing your first pair of notes?
- Do you want an explanation of how the app works, or do you just want to start immediately?

Put yourself there, Michael ‚Äî you tap the icon, the app opens. What do you *want* to happen?

### üßë Human (15:38:16)

The screen shows a prominent button "Start Training". Below that is a short paragraph describing the purpose of the app. Somewhere less prominent is a button that leads to the settings page.

### ü§ñ Assistant (15:38:23)

Clean and immediate. No onboarding wizard, no tutorial ‚Äî just go.

So the app gets out of the way and lets you start. The first pair of notes plays as soon as you tap "Start Training" ‚Äî or does it wait for another tap?

And once you're in the training screen:

- What do you see? Two buttons for "higher" and "lower"? 
- What happens after you answer ‚Äî do you get immediate feedback (correct/wrong)? A sound? A color change? Or does it just move to the next pair silently?
- Is the visualization of your perceptual profile visible *during* training, or is that a separate screen you check between sessions?

Paint the training screen for me. You've tapped "Start Training" ‚Äî what's in front of you?

### üßë Human (15:54:31)

Just go. I want the training to be as unobtrusive and quick as possible. As soon as the user starts training, the first note plays. There are two buttons on the screen, "Higher" and "Lower", or symbols indicating the same. The buttons are there all the time, but while the first note plays, they are visibly disabled. When the second note starts, they are enabled immediately.
Each note plays for about 1 second. This could be made into a user changeable setting. There is no overlap between notes and their is no pause betwwen them (although this could be made configurable in the future).
Immediately after the user presses a button, feedback is shown. This could be "thumbs up"/"thumbs down" for a short duration. There could be haptic feedback when the answer was wrong.

There should be a "Stop Training" button on the page, too. Preferably just a symbol. The should be no buttons to replay or skip the question. The training flow should feel very lightweight and enable quick progression through lots of note pairs. Answering should be instinctive, not after careful deliberation. Getting a single question right or wrong should feel inconsequential.

When the user stops training, the start page is shown again. The start page could show a stylized outline of the perceptual profile and on clicking it, a page with a more detailed visualization should be shown.

### ü§ñ Assistant (15:54:47)

This is beautifully clear. The design philosophy is really coming through: **lightweight, instinctive, low-stakes, high-volume.** Like a metronome for your ears ‚Äî tap tap tap, keep going.

Let me capture what just emerged:

**Training Screen:**
- Two buttons: Higher / Lower (or symbolic equivalents)
- Buttons visually disabled during first note, enabled when second note starts
- Each note ~1 second (configurable in settings)
- No overlap, no pause between notes (pause configurable in future)
- No replay, no skip ‚Äî keeps momentum
- Stop button (icon only) ‚Äî returns to start page
- Immediate visual feedback: thumbs up/down briefly
- Haptic feedback on wrong answer
- The feel: instinctive, inconsequential per question, high throughput

**Start Page:**
- Prominent "Start Training" button
- Short app description paragraph
- Settings button (less prominent)
- Stylized perceptual profile outline ‚Äî tappable to see detailed visualization

**Design Philosophy:**
- Each answer should feel like a reflex, not a decision
- Getting one wrong is no big deal ‚Äî the app is quietly learning from the pattern
- Volume of responses matters more than any single response

---

**Scenario 2: Three Weeks In**

You've been using the app regularly. You open it and see your perceptual profile on the start page. You've gotten noticeably better in the mid-range around C4 but you can see some weak areas.

- What does that profile visualization actually look like to you? A curve? A heat map? Colored bars per note?
- What information do you want from a glance vs. from the detailed view?
- At this point, do you feel the need to adjust any settings, or is the algorithm handling things well on its own?
- Has anything about the flow started to feel tedious or missing?

You've settled into a routine with this app. What's the experience like?

### üßë Human (16:14:21)

Here's an idea for the visualization: A piano keyboard overlaid with a confidence band plot. When I tap on a point in the plot, a popup shows more detailed statistics. We need to determine later what is appropriate.

### ü§ñ Assistant (16:14:33)

That's a strong visual metaphor ‚Äî the piano keyboard gives immediate musical context, and the confidence band shows at a glance where you're strong and where the gaps are. A musician looks at that and instantly understands "I'm solid in the middle but shaky up here."

**Visualization (captured):**
- Piano keyboard as the X-axis ‚Äî notes grounded in musical reality
- Confidence band plot overlaid ‚Äî showing discrimination ability across the range
- Tappable points for detailed statistics (specifics TBD)
- Stylized/miniature version on the start page, full version on a dedicated page

---

**Let's stay in this "three weeks in" scenario a moment longer.**

You've been doing sessions ‚Äî maybe 5-10 minutes here and there. The algorithm has been steering you toward your weak spots.

- Do you want to know *how many* pairs you've answered? Total, today, this week? Or does that feel like gamification creeping in?
- If you hit a plateau ‚Äî say your weak spot around B5 isn't improving despite lots of training there ‚Äî do you want the app to tell you anything? Or just keep presenting pairs and trust the process?
- Do you ever want to **manually override** the algorithm? Like "I want to focus on the octave around A3 right now" ‚Äî or is that against the philosophy of letting the algorithm drive?

What's your instinct here?

### üßë Human (16:18:54)

Definitely not for the first version, but in the future it would be nice to be able to view progress. For instance with a succession of plots each aggregating data over a given period, such as a day, week, or month. 
Manual override could be a feature for version 2.
Statistics of total pairs etc. are fine.

### ü§ñ Assistant (16:19:02)

Good ‚Äî clear prioritization. Let me note:

**V1:**
- Statistics are fine (total pairs, etc.) ‚Äî factual data, not scores
- Single perceptual profile visualization (current state)

**Future:**
- Progress over time: series of plots aggregated by day/week/month
- Manual override of the algorithm's focus area

---

**Scenario 3: Returning After a Break**

It's been two weeks since you last opened the app. You tap the icon.

- Should the experience be any different from a regular session? Or just "Start Training" and the algorithm picks up where it left off?
- Your ability may have shifted during the break ‚Äî should the app do anything special to recalibrate, or just let the adaptive algorithm naturally figure it out as you answer?

### üßë Human (16:20:00)

No, nothing special.

### ü§ñ Assistant (16:20:07)

Perfect. The algorithm handles it ‚Äî no "welcome back" fuss, no recalibration mode. Just pick up and go. Consistent with the overall philosophy.

---

**Scenario 4: The Settings Page**

You've tapped the settings button. What do you expect to find there?

Based on what we've discussed so far, I'm thinking:

- Note duration (default ~1 second)
- Note range (manual selection or adaptive mode)
- The "natural vs. mechanical" algorithm slider
- Reference pitch (A4 = 440Hz, adjustable)
- Sound source (sine wave for now, extensible later)

Anything else you'd want there? And is there anything you'd deliberately *keep out* of settings to avoid complexity?

### üßë Human (16:20:33)

That seems fine.

### ü§ñ Assistant (16:20:39)

Good. Settings are lean and purposeful ‚Äî nothing extra.

---

**Scenario 5: The Physical Context**

You're using this app in real life. Let's think about *where and how*:

- Are you using headphones or the device speaker? Does the app need to care about this?
- Portrait or landscape orientation?
- iPhone only, or iPad too? Does the layout need to adapt?
- Could you imagine using this during a practice break ‚Äî instrument in hand, phone on the music stand? Does that affect button size or placement?

### üßë Human (16:25:14)

Device speaker and headphones should just work. As for the orientation, I'm thinking mostly of portrait, but landscape should work, too. iPhone and iPad should be supported. As the UI is mostly empty so far, we don't need to waste space; on iPad the app should be shrinkable to a window.

I can very well imagine using the app incidentally. In a practice break, when waiting in line or when sitting on the bus. That's part of the point in making it as unobtrusive as possible. Squeeze in a bit of training here and there.

### ü§ñ Assistant (16:25:26)

That use case nails the design philosophy perfectly. This is a **pocket trainer** ‚Äî pull it out, do 30 seconds of pairs, put it away. Like flashcards for your ears.

**Captured:**
- Speaker and headphones ‚Äî no special handling needed
- Portrait primary, landscape supported
- iPhone and iPad, with iPad supporting windowed/compact mode
- UI should work for one-handed, casual use ‚Äî bus, queue, practice break
- Button sizing should accommodate quick, imprecise taps

---

I think we've covered the Role Playing technique really well. We walked through first launch, the training loop, settled use, returning after a break, settings, and physical context. The picture is cohesive.

Let me summarize what Role Playing surfaced before we move on:

**Key Requirements from Role Playing:**
- Start page: one prominent button, profile preview, settings access
- Training screen: minimal, two buttons + stop, immediate feedback, haptic on wrong
- No replay/skip ‚Äî keeps flow fast and instinctive
- Visualization: piano keyboard with confidence band plot, tappable for detail
- Statistics: factual (total pairs etc.), no scores
- Settings: note duration, range, algorithm slider, reference pitch, sound source
- Supports portrait + landscape, iPhone + iPad (windowed)
- Designed for incidental use ‚Äî quick sessions anywhere

**Ready to move to Morphological Analysis?** This is where we systematically map everything across dimensions to find gaps. It should tie the whole picture together nicely.

### üßë Human (16:25:52)

Go ahead.

### ü§ñ Assistant (16:26:12)

**Let's transition to Morphological Analysis!**

This technique takes everything we've uncovered and lays it out in a structured grid ‚Äî dimension by dimension ‚Äî to spot gaps and make sure we haven't missed anything.

**Technique: Morphological Analysis**
**Focus:** Systematic completeness check across all requirement dimensions
**Energy:** Analytical, methodical

---

Let me propose the key dimensions and what we've captured for each. I want you to challenge, add, or correct as we go.

**Dimension 1: Sound Engine**

| Parameter | V1 | Future |
|---|---|---|
| Timbre | Sine wave | Sampled sounds, MIDI out |
| Note duration | ~1s, configurable | ‚Äî |
| Note overlap | None | ‚Äî |
| Pause between notes | None | Configurable |
| Precision | Fractional cents (0.1) | ‚Äî |
| Reference pitch | A4=440Hz, configurable | ‚Äî |

**Dimension 2: Adaptive Algorithm**

| Parameter | V1 | Future |
|---|---|---|
| Input | User's answer history | ‚Äî |
| Output | Next note pair selection | ‚Äî |
| On correct | Tunable: harder nearby vs. jump to weak spot | ‚Äî |
| On wrong | Tunable: easier nearby vs. jump to weak spot | ‚Äî |
| Weak spot selection | Random, weighted by weakness | ‚Äî |
| Starting distance | 100 cents (1 semitone) | ‚Äî |
| Minimum distance | ~1 cent | ‚Äî |
| User control | "Natural vs. mechanical" slider | Manual focus override |
| Note range | Configurable or adaptive from C3-C5 | ‚Äî |
| Range expansion | When performance at edges passes threshold | ‚Äî |
| Decay handling | Implicit via algorithm | ‚Äî |

**Dimension 3: User Interface**

| Parameter | V1 | Future |
|---|---|---|
| Start page | Start button, app description, settings link, profile preview | ‚Äî |
| Training screen | Higher/Lower buttons, stop button, feedback | ‚Äî |
| Feedback | Thumbs up/down visual, haptic on wrong | ‚Äî |
| Visualization | Piano keyboard + confidence band, tappable | Progress over time plots |
| Statistics | Total pairs, factual data | Daily/weekly/monthly aggregation |
| Orientation | Portrait + landscape | ‚Äî |
| Devices | iPhone + iPad (windowed) | ‚Äî |

**Dimension 4: Data & Persistence**

| Parameter | V1 | Future |
|---|---|---|
| Perceptual profile | Persisted locally | ‚Äî |
| Answer history | Stored for algorithm | ‚Äî |
| Session state | Seamless resume | ‚Äî |
| Settings | Persisted locally | ‚Äî |

---

Looking at this grid, a few gaps jump out at me:

- **Audio interruptions** ‚Äî what happens if a phone call comes in mid-pair, or you unplug headphones? Does the pair just get discarded?
- **Data model** ‚Äî what exactly gets stored per answer? The two notes, the cent distance, the answer, correct/wrong, timestamp?
- **Onboarding data** ‚Äî on first ever launch, the algorithm has zero history. Does it just start with random pairs at 100 cents across the default range and learn from there?
- **Accessibility** ‚Äî any considerations? The app is inherently audio-dependent, but should labels and contrast be accessible?

What stands out to you? Any dimensions missing entirely?

### üßë Human (16:30:45)

Interruptions: yes, just discard the pair.
Data model: the two notes, correct/wrong, timestamp
Onboarding: yes, just start randomly with a semitone distance and consider every untouched note a weak spot
Accessibility: let's go for the low-hanging fruit first

### ü§ñ Assistant (16:30:56)

Clean answers across the board. Let me update the grid:

**Dimension 4: Data & Persistence (updated)**

| Parameter | V1 | Future |
|---|---|---|
| Per-answer record | Two notes, correct/wrong, timestamp | ‚Äî |
| Perceptual profile | Derived from answer history | ‚Äî |
| Session state | Seamless resume, discard interrupted pairs | ‚Äî |
| Cold start | Random pairs, 100 cents, all notes = weak | ‚Äî |
| Settings | Persisted locally | ‚Äî |

**Dimension 5: Accessibility**

| Parameter | V1 | Future |
|---|---|---|
| Approach | Low-hanging fruit ‚Äî labels, contrast, VoiceOver basics | Deeper a11y |

---

Let me probe a few more dimensions I think we haven't explicitly covered:

**Dimension 6: Technology & Architecture**

- Native Swift/SwiftUI, or a cross-platform framework?
- Is there any reason to have a backend/server, or is this entirely on-device?
- Any data export needs ‚Äî like exporting your training history as CSV for your own analysis?
- Minimum iOS version target?

**Dimension 7: Audio Generation**

- For sine wave generation ‚Äî are you thinking AVAudioEngine, or something simpler like AudioToolbox?
- Do the notes need smooth attack/release envelopes to avoid clicks, or is raw sine acceptable?
- Volume control ‚Äî just system volume, or an in-app setting?

What's your thinking on the technical side?

### üßë Human (16:38:36)

An addition: settings and data should optionally be stored in iCloud and synchronized across devices.

Tech: Swift and SwiftUI at their latest iteration. Remember, an important goal of this project is for me to learn about iOS development.
Target platform: iOS 26 is fine.
CSV export would be nice.

Audio generation: playback needs to be smooth so that it does not cause a distraction. In order to support samples and external sound generators, the concrete generation needs to be easily exchangeable, even while the app is running. AudioKit might be another candidate, if it carries its weight.

No in-app volume control is needed.

### ü§ñ Assistant (16:38:46)

Excellent. These are important architectural decisions. Let me capture them:

**Dimension 6: Technology & Architecture**

| Parameter | V1 | Future |
|---|---|---|
| Language/Framework | Swift / SwiftUI (latest) | ‚Äî |
| Target | iOS 26 | ‚Äî |
| Backend | None ‚Äî entirely on-device | ‚Äî |
| Storage | Local + optional iCloud sync | ‚Äî |
| Data export | CSV export of training history | ‚Äî |

**Dimension 7: Audio Engine**

| Parameter | V1 | Future |
|---|---|---|
| Generation | Smooth playback, no clicks (proper envelopes) | Samples, MIDI out |
| Architecture | Swappable sound source ‚Äî exchangeable at runtime | ‚Äî |
| Candidates | AVAudioEngine, possibly AudioKit (if justified) | ‚Äî |
| Volume | System volume only | ‚Äî |

---

The swappable sound source at runtime is a key architectural requirement ‚Äî it means the audio layer needs a clean protocol/interface from day one, even if V1 only has one implementation (sine).

Let me check for remaining gaps. Looking across all seven dimensions:

- **Localization** ‚Äî English only, or do you want to prepare for German or other languages?
- **App name** ‚Äî do you have one? The project folder is "peach" ‚Äî is that the name?
- **Testing** ‚Äî given that this is a learning project, do you want to include unit tests for the algorithm as a requirement? The adaptive algorithm seems like the most testable and most critical piece.

### üßë Human (16:41:56)

Localization: English and German are fine for the start.
App name: I'm having difficulty coming up with a good name and I would appreciate your help. I chose "Peach" because it sounds similar to "Pitch".
Testing is non-negotiable. The app is to be developed test first. Not just for the algorithmic core, Everything, as far as it is possible on the platform.

### ü§ñ Assistant (16:42:05)

Love the commitment to test-first. And "Peach" is a charming working title ‚Äî the pitch/peach phonetic connection works. We can revisit naming later if you want to brainstorm alternatives.

**Updated requirements:**

| Parameter | V1 |
|---|---|
| Localization | English + German |
| Working name | Peach |
| Testing | Test-first development, comprehensive coverage |

---

I think we've done a thorough job across all three techniques. Let me do a final sweep ‚Äî is there anything we haven't touched that's nagging at you? Anything about:

- How the app **sounds** beyond the notes (UI sounds, or silence?)
- **Dark mode / light mode** preferences?
- **Haptics** beyond wrong-answer feedback?
- Anything about how the **perceptual profile is calculated** from the raw answer data?

Or do you feel we've covered the ground well and are ready to organize everything into a coherent requirements document?

### üßë Human (16:43:13)

I think we've covered everything to go ahead.

### ü§ñ Assistant (16:43:17)

Let me save our technique execution results and load the next step.

### ü§ñ Assistant (16:43:49)

Now let me organize everything from our session and write it to the document.

**Outstanding creative work, Michael!** We've systematically explored your app's requirements through three complementary techniques.

**Session Achievement Summary:**
- **Creative Techniques Used:** Question Storming, Role Playing, Morphological Analysis
- **Session Focus:** Core requirements for a pitch discrimination ear training app

---

**Your Requirements ‚Äî Organized by Theme:**

**Theme 1: Core Training Loop**
- Sequential note pairs, no overlap, no pause, ~1s per note
- User answers Higher or Lower ‚Äî instinctive, low-stakes
- Immediate feedback: thumbs up/down visual, haptic on wrong
- No replay, no skip ‚Äî high throughput flow
- No gamification, no sessions/trials/attempts ‚Äî continuous training, stop anytime

**Theme 2: Adaptive Algorithm**
- Builds a perceptual profile from answer history (two notes, correct/wrong, timestamp)
- On correct: tunable ratio of "harder nearby" vs. "jump to weak spot"
- On wrong: tunable ratio of "easier nearby" vs. "jump to weak spot"
- Weak spots selected randomly, weighted by weakness
- Cold start: random pairs at 100 cents, all notes treated as weak
- Fractional cents (0.1), practical floor ~1 cent
- User-facing "natural vs. mechanical" slider controlling multiple parameters
- Primary axis: narrowing the cent gap; note range expansion is secondary

**Theme 3: Note Range & Musical Model**
- Musical notes, not frequencies ‚Äî A4=440Hz configurable reference
- Configurable range or adaptive starting from octave around C4
- Adaptive expansion when performance at edges passes threshold
- Cents as the unit (100 cents = 1 semitone)

**Theme 4: Audio Engine**
- Smooth playback with proper attack/release envelopes
- V1: sine wave generation
- Architecture: swappable sound source protocol, exchangeable at runtime
- Candidates: AVAudioEngine, possibly AudioKit
- Future: sampled sounds, MIDI output
- System volume only, no in-app control
- Interrupted pairs discarded silently

**Theme 5: User Interface & Experience**
- Start page: prominent "Start Training" button, short description, settings link, profile preview
- Training screen: Higher/Lower buttons (disabled during first note, enabled on second), stop icon
- Visualization: piano keyboard with confidence band plot, tappable for detail
- Statistics: factual data (total pairs etc.), no scores
- Settings: note duration, note range, algorithm slider, reference pitch, sound source
- Designed for incidental use ‚Äî bus, queue, practice break
- One-handed, quick-tap friendly

**Theme 6: Technology & Architecture**
- Swift / SwiftUI (latest), iOS 26
- Entirely on-device, no backend
- Local persistence + optional iCloud sync across devices
- CSV export of training history
- Test-first development, comprehensive coverage
- Portrait + landscape, iPhone + iPad (windowed on iPad)
- English + German localization

**Theme 7: Future Roadmap**
- Progress visualization over time (daily/weekly/monthly plots)
- Manual algorithm focus override
- Configurable pause between notes
- Sampled sounds and MIDI output
- Deeper accessibility

**Breakthrough Concept:**
- **"Training, not testing"** ‚Äî the fundamental design philosophy. The app doesn't assess you and give a score. It builds a perceptual map of your hearing and relentlessly trains your weak spots. Every interaction makes you better; no single answer matters.

---

**Does this capture everything accurately? Any themes or requirements you'd like to adjust or add before I finalize the session document?**

- **[C] Complete** ‚Äî Finalize the session document
- **[Adjust]** ‚Äî Modify or add to the requirements

### üßë Human (16:43:59)

C

### ü§ñ Assistant (16:46:28)

**Brainstorming session complete, Michael.**

Your session document is saved at `docs/brainstorming/brainstorming-session-2026-02-11.md`. It contains:

- Full project context and design philosophy
- Technique execution narrative (Question Storming, Role Playing, Morphological Analysis)
- Organized requirements across 7 themes: Core Training Loop, Adaptive Algorithm, Musical Model, Audio Engine, UI/UX, Technology & Architecture, and Future Roadmap
- Clear V1 scope separated from future enhancements

The next natural step would be turning these brainstorming results into a formal requirements or product specification document ‚Äî but that's for whenever you're ready.

### üßë Human (16:48:02)

I've already created a git repo. Please add all changed files and write a useful commit message.

### ü§ñ Assistant (16:48:40)

Committed as `7c1eb9b`. Clean working tree.

---

[‚Üê Previous (7bb2a2e9e8)](7bb2a2e9e8.md) | [Index](index.md) | [Next (1706776b0c) ‚Üí](1706776b0c.md)
