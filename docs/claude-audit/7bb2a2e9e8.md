---
commit: "7bb2a2e9e86bec7827ca60ca771c2e088646d1ec"
date: "2026-02-12T10:50:41+01:00"
author: "Michael Sch√ºrig <michael@schuerig.de>"
subject: "Add comprehensive PRD and glossary for Peach"
sessions: ["8bbeb49b-ba76-4437-9e43-0d7dbf4387c0"]
prev: "0a89329199.md"
next: "7c1eb9b60e.md"
---

[‚Üê Previous (0a89329199)](0a89329199.md) | [Index](index.md) | [Next (7c1eb9b60e) ‚Üí](7c1eb9b60e.md)

# Commit 7bb2a2e9e8

**Date:** 2026-02-12 10:50:41 UTC+01:00  
**Author:** Michael Sch√ºrig <michael@schuerig.de>

## Commit message

Add comprehensive PRD and glossary for Peach

Complete Product Requirements Document covering executive summary,
success criteria, scoping (MVP/Growth/Vision), five user journeys,
mobile app requirements, 42 functional requirements across 8 capability
areas, and non-functional requirements. Includes separate glossary
defining all product terms (concepts, screens, controls).

Co-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>

## Changed files

```
docs/planning-artifacts/glossary.md |  36 ++++
 docs/planning-artifacts/prd.md      | 333 ++++++++++++++++++++++++++++++++++++
 2 files changed, 369 insertions(+)
```

## Session `8bbeb49b‚Ä¶`

*2026-02-12 08:41:18 UTC ‚Üí 2026-02-12 09:51:10 UTC*

### üßë Human (08:41:18)

<command-message>bmad-agent-bmm-pm</command-message>
<command-name>/bmad-agent-bmm-pm</command-name>

### ü§ñ Assistant (08:41:31)

Hey Michael! üìã John here, your Product Manager.

Ready to dig into what actually matters for your product. Whether we're shaping a new PRD from scratch, validating what you've got, or getting your epics lined up for implementation ‚Äî I'm here to ask the hard questions and make sure we're building the right thing.

Here's what I can help you with:

1. **[MH] Menu Help** ‚Äî Redisplay this menu
2. **[CH] Chat** ‚Äî Chat with me about anything
3. **[CP] Create PRD** ‚Äî Expert-led facilitation to produce your Product Requirements Document
4. **[VP] Validate PRD** ‚Äî Validate a PRD is comprehensive, lean, well-organized and cohesive
5. **[EP] Edit PRD** ‚Äî Update an existing Product Requirements Document
6. **[CE] Create Epics & Stories** ‚Äî Create the Epics and Stories listing that will drive development
7. **[IR] Implementation Readiness** ‚Äî Ensure PRD, UX, Architecture, and Epics/Stories are all aligned
8. **[CC] Course Correction** ‚Äî Determine how to proceed if major changes are needed mid-implementation
9. **[PM] Party Mode** ‚Äî Start Party Mode
10. **[DA] Dismiss Agent** ‚Äî Exit

You can type `/bmad-help` at any time to get advice on what to do next. You can combine it with what you need help with, for example: `/bmad-help where should I start with an idea I have that does XYZ`

What would you like to do?

### üßë Human (08:42:40)

cp

### ü§ñ Assistant (08:42:45)

**Create Mode: Creating a new PRD from scratch.**

Let me load up the first step.

### ü§ñ Assistant (08:42:50)

Alright Michael, let's get this PRD built. Let me check the current state and discover what we're working with.

### ü§ñ Assistant (08:43:02)

Good ‚Äî no existing PRD, so this is a fresh workflow. Here's what I found:

**Input Document Discovery Results:**

- **Product briefs:** 0 files (none found)
- **Research:** 0 files (none found)
- **Brainstorming:** 1 file ‚Äî `docs/brainstorming/brainstorming-session-2026-02-11.md`
- **Project docs:** 0 files (none found ‚Äî greenfield project)
- **Project context:** Not found

So I've got one brainstorming session document to work with. Before I load it up and set everything up, let me confirm with you:

**Does this look right?** Is `brainstorming-session-2026-02-11.md` the right input, and do you have any other documents you'd like me to include ‚Äî maybe a brief, research notes, or anything else? Or are we good to go with what we've got?

### üßë Human (08:43:39)

yes

### ü§ñ Assistant (08:43:46)

Great stuff ‚Äî that brainstorming session is loaded with solid material. Now let me set up the PRD workspace.

### ü§ñ Assistant (08:44:35)

Welcome Michael! I've set up your PRD workspace for Peach.

**Document Setup:**
- Created: `docs/planning-artifacts/prd.md` from template
- Initialized frontmatter with workflow state

**Input Documents Discovered:**
- Product briefs: 0 files (none found)
- Research: 0 files (none found)
- Brainstorming: 1 file ‚úì loaded
- Project docs: 0 files (none found ‚Äî greenfield project)

**Files loaded:**
- `docs/brainstorming/brainstorming-session-2026-02-11.md` ‚Äî Core requirements brainstorming session covering training philosophy, adaptive algorithm, UI, audio engine, and technology stack

This is a **greenfield project** ‚Äî we're building from scratch. That brainstorming session is excellent input. You've already got a sharp design philosophy ("training, not testing") and a well-defined core loop. My job now is to pressure-test all of that, find the gaps, and turn it into a rigorous PRD.

---

**[C] Continue** ‚Äî Save this and move to Project Discovery (Step 2 of 11)

### üßë Human (08:44:56)

c

### ü§ñ Assistant (08:45:19)

From step 1, I have loaded:
- Product briefs: 0
- Research: 0
- Brainstorming: 1
- Project docs: 0

This is a greenfield project ‚Äî defining the full product from scratch.

Now, Michael, I've already got a solid picture from your brainstorming session, so let me cut to the chase rather than asking you things you've already answered.

Here's what I'm hearing:

- **Project Type:** **Mobile App** ‚Äî native iOS (Swift/SwiftUI), targeting iPhone and iPad, entirely on-device. Classic mobile app signals all over this.
- **Domain:** **EdTech / Music Training** ‚Äî it's a learning tool, but a very specific kind: perceptual skill training for musicians. No students, no curriculum, no LMS ‚Äî so it's edtech-adjacent but lighter on compliance concerns. No COPPA/FERPA issues since this isn't aimed at minors in educational settings.
- **Complexity:** **Low** ‚Äî no backend, no regulated industry, no multi-tenancy, no payment processing, single primary user (you). The algorithmic challenge is real but the product complexity is deliberately minimal.
- **Project Context:** **Greenfield** ‚Äî building from scratch.

Does this classification feel right to you, or would you adjust anything? For instance, would you bump complexity to medium given the adaptive algorithm and audio engine work, or do you agree that the deliberate simplicity of the product keeps it low?

### üßë Human (08:46:11)

Yes, it's fine.

### ü§ñ Assistant (08:46:14)

Got it. Here's the confirmed classification:

**Project Type:** Mobile App
**Domain:** EdTech / Music Training
**Complexity:** Low
**Project Context:** Greenfield

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Product Vision (Step 3 of 11)

### üßë Human (08:46:27)

c

### ü§ñ Assistant (08:48:12)

Good ‚Äî classification saved. Moving to Step 3: Success Criteria.

**Progress: Step 3 of 11** ‚Äî Next: User Journey Mapping

---

Michael, before I start asking questions, let me pull from what you've already told me in the brainstorming session. You've actually laid out some pretty clear success signals already:

**What I'm seeing from your brainstorming:**

- **User success** is about perceptual improvement ‚Äî actually getting better at hearing pitch differences. The "aha" moment is when you notice your weak spots shrinking on the profile visualization.
- **The differentiator** is crystal clear: "training, not testing." No scores, no gamification, no failure states. Continuous adaptive improvement.
- **Technical success** signals: test-first development, swappable audio architecture, smooth playback with no clicks.
- You also have **three personal learning goals** baked in: improve pitch perception, learn iOS/SwiftUI, explore AI-assisted development.

But here's where I need to push you a bit ‚Äî some of this is still fuzzy:

1. **How would you know if your pitch perception is actually improving?** Is it "I can now reliably detect 5-cent differences where I used to need 20 cents"? Is there a concrete threshold you'd consider a win? Or is it more about the feel ‚Äî noticing better intonation in your own playing?

2. **For the learning goals** (iOS dev, AI-assisted dev) ‚Äî what does success look like there? Ship to the App Store? Have a working app on your phone? Something else?

3. **What's the minimum the app needs to do before you'd actually use it regularly?** I want to nail the MVP boundary. For example ‚Äî do you need the visualization to start training, or would just the training loop with the adaptive algorithm be enough to pull you in?

### üßë Human (08:59:19)

I would know that pitch perception is improving by looking at consecutive visualizations of the perceptual profile. To make this explicit, the visualization will probably have to be enhanced to show development over time. Note: I don't think there's one single, scalar metric of improvement.

My learning goals are rather vague and I'm fine with that. I just want to get started.

I do not need the visualization, for me personally, looking at the raw data would be enough to use the app. However, that's not something other users would tolerate. So, for the MVP, the visualilzation is required. Including visualization of development over time.

### ü§ñ Assistant (08:59:34)

Interesting ‚Äî you just pulled "progress visualization over time" from the future roadmap into MVP. That's a meaningful scope decision and I think it's the right call. If the app's whole philosophy is "training, not testing," users need to *see* that the training is working. Without temporal visualization, they're flying blind.

A couple of follow-ups before I draft:

1. **"Not a single scalar metric"** ‚Äî I buy that completely for the *user experience*. But for *you as the product owner*, how would you judge whether the adaptive algorithm is actually working? Would you look at something like "the average detectable cent difference across all notes is trending downward over weeks"? I'm not asking for a score to show users ‚Äî I'm asking what would make you say "yes, the algorithm is doing its job."

2. **Business success** ‚Äî you said this isn't a commercial product, and your learning goals are intentionally vague. So let me reframe: is there a point where you'd consider the *project* a success even if you stopped working on it? For example, "it's on my phone and I use it 3x a week" ‚Äî or is success just "I built it and learned things along the way"?

3. **Technical success** ‚Äî you were explicit about test-first development being non-negotiable. Any other hard technical lines? For instance, is "runs smoothly on my device" enough, or do you care about performance benchmarks like audio latency thresholds?

### üßë Human (09:08:13)

The algorithms has a set of parameters and they will have to be tuned during development and testing by me. Possibly, I will discovers further parameters in the process. In general, yes, my expectation is that with training the noticeable difference will go down. If that is not the case, then something is probably wrong.

I'd consider the project a success when I have the app on my iPhone and it works according to the MVP scope.

Technical success: apart from test-first development, I take a modern, modular architecture as a given in any project. Backward compatibility, with earlier app versions or iOS versions, is not an issue, therefore we can and should use all the latest and greatest frameworks and libraries.

### ü§ñ Assistant (09:08:33)

Sharp and pragmatic. Let me draft the success criteria and scope based on everything we've discussed.

---

**Success Criteria (Draft)**

### User Success

- The perceptual profile shows measurable narrowing of detectable cent differences over time through continued training
- The training loop feels instinctive and low-friction ‚Äî users can do 30 seconds of pairs without thinking about the app, just about the sounds
- The temporal visualization makes improvement visible without reducing it to a single score
- Users return to the app regularly because it fits into incidental moments (practice breaks, commuting, waiting)

### Business Success

- The app is installed on Michael's iPhone and works according to MVP scope
- This is a personal/learning project ‚Äî no commercial metrics apply
- Secondary success: meaningful learning in iOS/SwiftUI development and AI-assisted workflows

### Technical Success

- Test-first development with comprehensive coverage ‚Äî non-negotiable
- Modern, modular architecture using latest Swift/SwiftUI and iOS 26 frameworks
- No backward compatibility constraints ‚Äî always target latest
- Adaptive algorithm parameters are tunable and discoverable during development
- Audio playback is smooth with proper envelopes (no clicks or artifacts)

### Measurable Outcomes

- Algorithm validation: average detectable cent difference trends downward with sustained training; if it doesn't, the algorithm needs adjustment
- Algorithm parameters are exposed for manual tuning during development/testing
- Training loop throughput: pairs can be answered at a reflexive pace without UI bottlenecks

---

**Product Scope (Draft)**

### MVP ‚Äî Minimum Viable Product

- Core training loop (sequential note pairs, higher/lower, immediate feedback, haptic on wrong)
- Adaptive algorithm with tunable parameters and cold start behavior
- Sine wave audio engine with proper envelopes
- Start screen, training screen, visualization screen, settings
- Perceptual profile visualization (piano keyboard + confidence band)
- Temporal progress visualization (development over time)
- Local persistence of all training data
- iPhone + iPad support, portrait + landscape
- English + German localization

### Growth Features (Post-MVP)

- iCloud sync across devices
- CSV export of training history
- Swappable sound sources (sampled sounds)
- Manual algorithm focus override (choose note range to train)
- Configurable pause between notes

### Vision (Future)

- MIDI output to external instruments/apps
- Deeper accessibility improvements beyond low-hanging fruit
- Additional sound sources and timbres

---

How does this look? Anything you'd move between scopes, add, or cut?

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to User Journey Mapping (Step 4 of 11)

### üßë Human (09:11:22)

c

### ü§ñ Assistant (09:11:38)

Now loading Step 4: User Journey Mapping.

### ü§ñ Assistant (09:11:51)

**Progress: Step 4 of 11** ‚Äî Next: Domain Requirements

---

Michael, your brainstorming session already did some solid role-playing work here ‚Äî five scenarios covering first launch, training flow, three weeks in, returning after a break, and physical context. That's a strong foundation.

But let me challenge the user type coverage. From what I can see, this app has essentially **one user type** ‚Äî the musician training their ear. There's no backend, no admin panel, no API consumers, no support staff. It's a single-user, on-device app.

So the question isn't "who else uses this?" ‚Äî it's "what are the meaningfully different *situations* the same user encounters?"

From your brainstorming, I'd propose these journey narratives:

1. **First Launch** ‚Äî musician picks up the app for the first time, zero history
2. **Daily Training** ‚Äî the core loop, quick incidental session
3. **Checking Progress** ‚Äî user wants to see if training is working (visualization)
4. **Returning After a Break** ‚Äî hasn't used the app in weeks, picks it back up
5. **Tweaking Settings** ‚Äî user adjusts algorithm behavior, note range, or sound

Before I draft these out as full narratives ‚Äî does this set of journeys cover the ground? Is there a scenario I'm missing that would surface different requirements? For example, anything around audio interruptions (phone call mid-training), or switching between iPhone and iPad?

### üßë Human (09:13:51)

Sounds fine. Go ahead.

### ü§ñ Assistant (09:14:42)

Here are the journey narratives:

---

## User Journeys

### Journey 1: First Launch ‚Äî "Just Let Me Start"

**Persona:** Sarah, a semi-professional cellist who's been told her intonation in the upper registers is inconsistent. She's tried InTune but hated the test-anxiety feel of it.

**Opening Scene:** Sarah downloads Peach after a frustrating orchestra rehearsal. She opens the app and sees a clean start screen with a prominent "Start Training" button, a brief description of what the app does, and a settings link she ignores. No sign-up, no tutorial, no onboarding wizard.

**Rising Action:** She taps "Start Training." Two notes play in sequence ‚Äî she taps "Higher." Thumbs up. Two more notes. "Lower." Thumbs up. Another pair ‚Äî she's wrong, feels a subtle haptic buzz, sees the thumbs down, but the next pair is already coming. Within 10 seconds she's in a rhythm. It feels like a reflex game, not a test.

**Climax:** After 2 minutes, she stops. There's no score screen, no "you got 73%." She just... stops. She checks the visualization ‚Äî it's sparse, mostly showing the cold-start state, but she can already see a few data points forming. It feels like the beginning of something, not a judgment.

**Resolution:** She puts her phone away thinking "that was easy." No cognitive overhead. She'll do it again tomorrow during her warm-up break.

**Requirements revealed:** Zero-friction onboarding, no account creation, immediate training access, cold start algorithm behavior, graceful sparse-data visualization.

---

### Journey 2: Daily Training ‚Äî "Thirty Seconds on the Bus"

**Persona:** Sarah, three weeks in. She's on the bus to a gig.

**Opening Scene:** She pulls out her phone, opens Peach, taps "Start Training." Earbuds in, one hand holding the grab rail.

**Rising Action:** Pairs come fast. She's tapping with her thumb ‚Äî the buttons are large enough that she doesn't need to look carefully. Higher. Lower. Higher. Higher. She gets one wrong and feels the haptic. The algorithm shifts ‚Äî the interval was tighter than usual, probing the edge of her perception around B5.

**Climax:** Her bus stop comes. She taps the stop icon mid-pair. The incomplete pair is silently discarded. No "are you sure?" dialog, no session summary, no guilt.

**Resolution:** Total elapsed time: 40 seconds. She answered maybe 15 pairs. The app recorded every answer. Tomorrow, the algorithm will pick up exactly where the profile left off.

**Requirements revealed:** One-handed operation, large tap targets, instant start/stop, no session boundaries, interrupted pair handling, algorithm continuity across sessions, haptic feedback, portrait mode usability.

---

### Journey 3: Checking Progress ‚Äî "Is This Actually Working?"

**Persona:** Sarah, six weeks in. She's curious whether she's improving.

**Opening Scene:** From the start screen, she taps the perceptual profile preview ‚Äî a stylized miniature of her pitch landscape.

**Rising Action:** The full visualization opens: a piano keyboard along the X-axis with a confidence band overlaid showing her current detection thresholds across the frequency range. She can see she's strong around A3-A4 (her most-played range) and weaker in the extremes. She taps a note to see detailed stats ‚Äî total pairs involving that note, current threshold, trend.

**Climax:** She switches to the temporal view. A series of profile snapshots over the past six weeks shows the confidence band tightening, especially around D5-G5 where she used to be weakest. The improvement is visible and undeniable ‚Äî not as a score going up, but as a landscape reshaping.

**Resolution:** She feels motivated. Not because she "leveled up" but because she can see her perception literally changing shape. She starts a quick training session targeting the areas that still look wide.

**Requirements revealed:** Tappable profile visualization, per-note detail view, temporal progress visualization (multiple snapshots over time), factual statistics (no scores), visual before/after contrast, navigation from start screen to visualization.

---

### Journey 4: Returning After a Break ‚Äî "Where Was I?"

**Persona:** Sarah, back after a three-week vacation with no training.

**Opening Scene:** She opens Peach. The start screen looks exactly the same. No "welcome back!" banner, no guilt trip about missed days, no streak counter reset.

**Rising Action:** She taps "Start Training." The algorithm doesn't reset or re-test. It picks up from her existing profile ‚Äî starting with areas it already knows are weak, at difficulty levels calibrated to her last known thresholds.

**Climax:** After a few pairs she notices she's getting more wrong answers than she used to. The algorithm naturally responds ‚Äî widening the intervals slightly, adapting to her current (slightly degraded) perception. No announcement, no "your skills have decreased" message. Just quiet recalibration.

**Resolution:** Within a few sessions over the next week, her profile tightens back up. The temporal visualization shows a small dip and recovery ‚Äî an honest record, not a judgment.

**Requirements revealed:** No special return-from-break logic, algorithm resilience to skill regression, no streak/engagement mechanics, profile persistence across long gaps, honest temporal data (showing dips too).

---

### Journey 5: Tweaking Settings ‚Äî "Make It Feel Different"

**Persona:** Sarah wants to adjust how the app behaves. The default feels a bit too mechanical ‚Äî pairs jump around the keyboard unpredictably.

**Opening Scene:** She navigates to settings from the start screen.

**Rising Action:** She finds the "Natural vs. Mechanical" slider. She moves it toward "Natural" ‚Äî pairs now tend to stay in nearby pitch regions more often before jumping to a weak spot. She also extends her note range upward since she's been working on higher positions. She adjusts note duration to be slightly shorter ‚Äî she wants to sharpen her reflexes.

**Climax:** She starts a training session with the new settings. It feels different immediately ‚Äî more musical, less random. The pairs flow in a way that feels closer to real playing.

**Resolution:** She finds her sweet spot after a couple of adjustments over the next few days. The algorithm is the same underneath, but the experience feels tailored to her.

**Requirements revealed:** Settings screen with algorithm slider, note range configuration, note duration configuration, reference pitch setting, sound source selection, immediate effect on training behavior, settings persistence.

---

### Journey Requirements Summary

| Capability Area | Journeys | Priority |
|---|---|---|
| Zero-friction start (no onboarding) | 1 | MVP |
| Core training loop (pairs, feedback, haptic) | 1, 2 | MVP |
| One-handed / large tap targets | 2 | MVP |
| Instant start/stop, no session boundaries | 2, 4 | MVP |
| Interrupted pair handling | 2 | MVP |
| Cold start algorithm | 1 | MVP |
| Adaptive algorithm with profile continuity | 2, 4 | MVP |
| Perceptual profile visualization | 3 | MVP |
| Temporal progress visualization | 3, 4 | MVP |
| Per-note detail view | 3 | MVP |
| Settings (slider, range, duration, pitch, sound) | 5 | MVP |
| Algorithm resilience to skill regression | 4 | MVP |
| No gamification / no guilt mechanics | 1, 4 | MVP (design constraint) |

---

All five journeys point to the same user type in different contexts, which is exactly right for this app. The requirements they surface are consistent with your brainstorming and MVP scope ‚Äî no surprises, but now grounded in concrete scenarios.

Anything you'd adjust, add, or cut?

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Domain Requirements (Step 5 of 11)

### üßë Human (09:14:59)

c

### ü§ñ Assistant (09:15:56)

Now loading Step 5: Domain Requirements.

### ü§ñ Assistant (09:16:02)

**Progress: Step 5 of 11** ‚Äî Next: Innovation Focus

The domain complexity from our discovery is **low** ‚Äî this is a personal, on-device music training app with no regulated data, no backend, no compliance concerns. We don't need deep domain-specific requirements here.

Would you like to:
- **[C]** Skip this step and move to Innovation
- **[D]** Do domain exploration anyway

### üßë Human (09:16:23)

c

### ü§ñ Assistant (09:16:42)

**Progress: Step 6 of 11** ‚Äî Next: Project Type Analysis

---

I'm scanning for innovation signals, and I actually do see something worth calling out here. This isn't about gesture innovation or AR/VR (the typical mobile app innovation signals) ‚Äî it's about the **training philosophy** itself.

The core innovation is the "training, not testing" paradigm. Existing pitch training apps like InTune follow a test-and-score model ‚Äî escalate difficulty until failure, report a number. Peach inverts that entirely: it builds a perceptual map and relentlessly targets weak spots. Every interaction improves the user; no single answer matters.

That's not a novel technology ‚Äî it's a novel *approach* to an existing problem category. The adaptive algorithm that builds a per-note perceptual profile and uses it to drive pair selection is the concrete manifestation of that philosophy.

My question to you: **do you see this as genuinely innovative ‚Äî something worth documenting as a differentiator ‚Äî or is it more of a well-executed take on adaptive learning that just hasn't been applied to pitch training yet?** That distinction matters for how we frame it in the PRD.

### üßë Human (09:17:37)

No, I don't think it is necessary to belabor this.

### ü§ñ Assistant (09:17:41)

Fair enough ‚Äî it's a clear design philosophy, not a research thesis. It's already well-captured in the success criteria and journeys. No need to add a separate section.

**[C]** Continue ‚Äî Skip innovation section and move to Project Type Analysis (Step 7 of 11)

### üßë Human (09:17:44)

c

### ü§ñ Assistant (09:18:01)

**Progress: Step 7 of 11** ‚Äî Next: Scoping

---

From the project-types CSV, the key questions for a mobile app are:

1. **Native or cross-platform?**
2. **Offline needed?**
3. **Push notifications?**
4. **Device features?**
5. **Store compliance?**

Your brainstorming session already answered most of these clearly, so let me confirm rather than re-ask:

1. **Native** ‚Äî Swift/SwiftUI, iOS 26. No cross-platform. Settled.
2. **Offline** ‚Äî entirely on-device, no backend at all. The app *is* offline by default.
3. **Push notifications** ‚Äî I don't see any mention of these. Given the "no guilt, no streaks" philosophy, I'd assume you don't want notification reminders nagging users to train. Correct?
4. **Device features** ‚Äî audio output (speaker/headphones), haptic engine. Anything else? Microphone for future use?
5. **Store compliance** ‚Äî you'll need to go through App Store review. No special content concerns (no user-generated content, no in-app purchases for MVP, no account system). Straightforward submission.

The one I want to push on: **push notifications**. Even without gamification, some users might *want* a gentle daily reminder. Is that a hard no, a "maybe post-MVP," or something you'd consider?

### üßë Human (09:18:50)

No push notifications.

### ü§ñ Assistant (09:19:04)

Crystal clear. No notifications, period. Fits the philosophy perfectly ‚Äî the app is there when you want it, silent when you don't.

Here's the mobile-specific requirements I'll add:

---

## Mobile App Specific Requirements

### Project-Type Overview

Native iOS application built with Swift/SwiftUI targeting iOS 26. Entirely on-device with no backend, no account system, no push notifications. Designed for iPhone and iPad with portrait as primary orientation and landscape supported.

### Platform Requirements

- Native iOS: Swift / SwiftUI (latest iteration)
- Target: iOS 26 minimum, no backward compatibility
- Devices: iPhone and iPad, with iPad supporting windowed/compact mode
- Orientations: Portrait (primary) + landscape
- No cross-platform considerations

### Device Permissions & Features

- Audio output (speaker and headphones) ‚Äî no microphone required
- Haptic engine (for wrong-answer feedback)
- No camera, location, contacts, or other permissions needed
- No push notification permission required
- System volume only ‚Äî no in-app audio controls

### Offline & Data

- Fully offline by design ‚Äî no network access required
- Local persistence for all training data (per-answer records)
- Optional iCloud sync (post-MVP)
- No account creation or authentication

### Store Compliance

- Standard App Store review ‚Äî no special content concerns
- No in-app purchases (MVP)
- No user-generated content
- No third-party authentication
- Privacy nutrition label: minimal data collection (training data stored locally only)
- English + German localization

### Implementation Considerations

- Swappable audio engine architecture (protocol/interface-based)
- Test-first development with comprehensive coverage
- Local data persistence model storing per-answer records (two notes, correct/wrong, timestamp)
- Audio interruption handling (phone call, headphone unplug ‚Üí discard incomplete pair)

---

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Scoping (Step 8 of 11)

### üßë Human (09:19:49)

c

### ü§ñ Assistant (09:20:11)

Now loading Step 8: Scoping.

### ü§ñ Assistant (09:20:29)

**Progress: Step 8 of 11** ‚Äî Next: Functional Requirements

---

Michael, looking at the PRD so far, I notice we already defined scope pretty thoroughly in Step 3 ‚Äî MVP, Growth, and Vision are clearly delineated. So rather than re-do that exercise, let me pressure-test the boundaries and check for gaps.

**Current MVP assessment:** This is a focused, well-scoped MVP. It's an **experience MVP** ‚Äî the core value proposition is the training *feel*, not a feature checklist. The minimum that makes this useful is: play pairs, answer higher/lower, adaptive algorithm picks the next pair intelligently, and you can see your profile developing over time.

**One scope tension I want to flag:**

You pulled temporal progress visualization into MVP. That's the right call for users, but it's also the most complex visualization work in the whole app. The core training loop is algorithmically interesting but UI-simple. The visualization is the opposite ‚Äî algorithmically straightforward but potentially significant UI/UX effort (aggregation logic, time-axis rendering, meaningful snapshot comparisons on a phone screen).

**My question:** Would you consider a simpler V1 of the temporal visualization for MVP? For example, a basic "your average threshold this week vs. last week" rather than full profile-snapshot comparisons from day one? Or do you want the full profile-over-time visualization as a hard MVP requirement?

**Risks I see:**

- **Technical risk:** The adaptive algorithm is the core intellectual challenge. Parameters will need tuning. Mitigation: you've already planned for this ‚Äî parameters are exposed and tunable.
- **No market risk:** This is a personal project. You're the user. You know the problem intimately.
- **Resource risk:** Solo developer learning a new platform. Mitigation: AI-assisted development is explicitly part of the project goals. Keep MVP lean.

What's your take on the visualization scope question?

### üßë Human (09:21:59)

Ok, just for the MVP boil it down to a single number and skip the complex visualization.

### ü§ñ Assistant (09:22:04)

Smart call. That's a real scope reduction ‚Äî you keep the signal ("am I improving?") without the UI complexity. The full temporal profile visualization moves to post-MVP.

Let me clarify: what single number are you thinking? Something like "average detectable cent difference" across all notes, shown as a trend? Or something else?

### üßë Human (09:24:33)

The arithmetic mean and standard deviation of detectable cent differences over the current training range.

### ü§ñ Assistant (09:24:53)

That's clean and meaningful ‚Äî two numbers that tell you both "how good" and "how consistent." And they're trivial to compute from the existing per-answer data. No fancy visualization needed.

Here's the updated scoping:

---

## Project Scoping & Phased Development

### MVP Strategy & Philosophy

**MVP Approach:** Experience MVP ‚Äî the core value is the training feel, not a feature count. Ship the smallest thing that lets a musician train pitch discrimination with adaptive pair selection and see that it's working.

**Resource:** Solo developer learning iOS/SwiftUI, AI-assisted development.

### MVP Feature Set (Phase 1)

**Core User Journeys Supported:**
- Journey 1 (First Launch), Journey 2 (Daily Training), Journey 4 (Return After Break), Journey 5 (Settings)
- Journey 3 (Checking Progress) supported in simplified form

**Must-Have Capabilities:**
- Core training loop (sequential pairs, higher/lower, immediate feedback, haptic on wrong)
- Adaptive algorithm with tunable parameters, cold start, profile continuity
- Sine wave audio engine with proper attack/release envelopes
- Start screen, training screen, settings screen
- Perceptual profile visualization (piano keyboard + confidence band ‚Äî current snapshot only)
- Summary statistics: arithmetic mean and standard deviation of detectable cent differences over current training range (shown as trend)
- Local persistence of all per-answer training data
- iPhone + iPad, portrait + landscape
- English + German localization
- Audio interruption handling (discard incomplete pair)

### Post-MVP Features (Phase 2 ‚Äî Growth)

- Full temporal progress visualization (profile snapshots over time)
- Per-note detail view with trend data
- iCloud sync across devices
- CSV export of training history
- Swappable sound sources (sampled sounds)
- Manual algorithm focus override (choose note range to train)
- Configurable pause between notes

### Future Features (Phase 3 ‚Äî Vision)

- MIDI output to external instruments/apps
- Deeper accessibility improvements
- Additional sound sources and timbres

### Risk Mitigation Strategy

**Technical risk:** Adaptive algorithm requires parameter tuning ‚Äî mitigated by exposing all parameters for manual adjustment during development. Algorithm is the intellectual core; UI is deliberately kept simple.

**Resource risk:** Solo developer on unfamiliar platform ‚Äî mitigated by AI-assisted development, latest frameworks (less legacy baggage), and lean MVP scope.

**No market risk:** Developer is the primary user and knows the problem domain intimately.

---

Note: this replaces the earlier Product Scope section since it's a more detailed version of the same decisions. I'll consolidate when saving.

Anything to adjust?

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Functional Requirements (Step 9 of 11)

### üßë Human (09:25:36)

c

### ü§ñ Assistant (09:25:43)

I'll update the frontmatter, replace the earlier Product Scope section with the refined scoping, and append the new section.

### ü§ñ Assistant (09:26:03)

Now loading Step 9: Functional Requirements.

### ü§ñ Assistant (09:26:37)

**Progress: Step 9 of 11** ‚Äî Next: Non-Functional Requirements

---

Michael, this is the big one. The functional requirements become the capability contract ‚Äî everything downstream (UX, architecture, epics) traces back to what we list here. If it's not in the FRs, it won't exist.

I've synthesized everything from our conversation ‚Äî success criteria, all five journeys, scoping decisions, mobile-specific requirements ‚Äî into a comprehensive FR list. Let me know what's missing or wrong.

---

## Functional Requirements

### Training Loop

- **FR1:** User can start a training session immediately from the start screen with a single tap
- **FR2:** User can hear two sequential notes played one after another within a pair
- **FR3:** User can answer whether the second note was higher or lower than the first
- **FR4:** User can see immediate visual feedback (thumbs up/down) after answering
- **FR5:** User can feel haptic feedback when answering incorrectly
- **FR6:** User can stop a training session at any time with a single tap
- **FR7:** System discards incomplete pairs when training is interrupted (user stop, phone call, headphone disconnect)
- **FR8:** System disables answer controls during the first note and enables them when the second note begins playing

### Adaptive Algorithm

- **FR9:** System selects the next note pair based on the user's perceptual profile
- **FR10:** System adjusts pair difficulty (cent difference) based on answer correctness ‚Äî narrower on correct, wider on wrong
- **FR11:** System balances between training nearby the current pitch region and jumping to weak spots, controlled by a tunable ratio
- **FR12:** System initializes new users with random pairs at 100 cents (1 semitone) with all notes treated as weak
- **FR13:** System maintains the perceptual profile across sessions without requiring explicit save or resume
- **FR14:** System supports fractional cent precision (0.1 cent resolution) with a practical floor of approximately 1 cent
- **FR15:** System exposes algorithm parameters for adjustment during development and testing

### Audio Engine

- **FR16:** System generates sine wave tones at precise frequencies derived from musical notes and cent offsets
- **FR17:** System plays notes with smooth attack/release envelopes (no audible clicks or artifacts)
- **FR18:** System uses the same timbre for both notes in a pair
- **FR19:** System supports configurable note duration
- **FR20:** System supports a configurable reference pitch (default A4 = 440Hz)

### Perceptual Profile & Statistics

- **FR21:** User can view their current perceptual profile as a visualization with a piano keyboard axis and confidence band overlay
- **FR22:** User can view a stylized preview of their perceptual profile on the start screen
- **FR23:** User can navigate from the start screen to the full profile visualization
- **FR24:** User can view summary statistics: arithmetic mean and standard deviation of detectable cent differences over the current training range
- **FR25:** User can see summary statistics as a trend (improving/stable/declining)
- **FR26:** System computes the perceptual profile from stored per-answer data

### Data Persistence

- **FR27:** System stores every answered pair as a record containing: two notes, correct/wrong, timestamp
- **FR28:** System persists all training data locally on-device
- **FR29:** System maintains data integrity across app restarts, backgrounding, and device reboots

### Settings & Configuration

- **FR30:** User can adjust the algorithm behavior via a "Natural vs. Mechanical" slider
- **FR31:** User can configure the note range (manual bounds or adaptive mode)
- **FR32:** User can configure note duration
- **FR33:** User can configure the reference pitch
- **FR34:** User can select the sound source (MVP: sine wave only)
- **FR35:** System persists all settings across sessions
- **FR36:** System applies setting changes immediately to subsequent training pairs

### Localization & Accessibility

- **FR37:** User can use the app in English or German
- **FR38:** System provides basic accessibility support (labels, contrast, VoiceOver basics)

### Device & Platform

- **FR39:** User can use the app on iPhone and iPad
- **FR40:** User can use the app in portrait and landscape orientations
- **FR41:** User can use the app in iPad windowed/compact mode
- **FR42:** User can operate the training loop one-handed with large, imprecise-tap-friendly controls

---

That's 42 FRs across 8 capability areas. Every capability from the MVP scope, every journey requirement, and every mobile-specific need is covered.

**Key things to gut-check:**
- FR11 captures the "Natural vs. Mechanical" slider's *effect* on the algorithm, while FR30 captures the user's *ability to adjust* it. Both needed.
- FR34 lists sound source selection even though MVP is sine-only ‚Äî this keeps the architecture honest for the swappable engine.
- FR25 (trend on summary stats) ‚Äî is "improving/stable/declining" enough, or do you want actual historical values?

This is the capability contract. Anything missing, wrong, or over-specified?

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Non-Functional Requirements (Step 10 of 11)

### üßë Human (09:32:20)

This looks all good, but before we continue, I'd like to go into naming things. Most terms are suitable, but I don't like talking to users about "note pairs". We need a better term for that. Also, I'd like you to create a glossary.

### ü§ñ Assistant (09:32:32)

Good instinct ‚Äî naming matters. If users see "note pairs" in the UI or documentation, it sounds clinical and technical. Let's find something better.

The interaction is: two notes played in sequence, user judges the relationship. What matters to the user is the *comparison*, not the pair structure.

Some options to kick around:

1. **"Comparison"** ‚Äî neutral, accurate. "You've completed 847 comparisons." 
2. **"Round"** ‚Äî implies a game though, which clashes with the no-gamification philosophy.
3. **"Exercise"** ‚Äî implies deliberate effort, not the reflexive 30-second-on-the-bus feel you want.
4. **"Listen"** ‚Äî verb-forward, natural. "1,200 listens." But slightly ambiguous.
5. **"Challenge"** ‚Äî too gamified.
6. **"Interval"** ‚Äî musically accurate (it *is* an interval comparison), but overloaded. Musicians know "interval" as a specific concept (major third, perfect fifth), and this is about *size* discrimination, not identification.

My recommendation: **"comparison"**. It's precise, non-gamified, and scales naturally ‚Äî "comparisons completed," "your last 50 comparisons," etc. It also maps to what the user is actually doing: comparing two pitches.

What resonates with you? Or do you have something else in mind?

### üßë Human (09:33:12)

Let's go with "comparison".

### ü§ñ Assistant (09:33:26)

Good. Now let me pull together a glossary from everything we've established. Here's my draft:

---

## Glossary

| Term | Definition |
|---|---|
| **Comparison** | A single training interaction: two sequential notes played, user judges whether the second is higher or lower than the first. The atomic unit of training. |
| **Perceptual Profile** | A map of the user's pitch discrimination ability across their training range, showing the smallest detectable cent difference at each note. Visualized as a confidence band over a piano keyboard. |
| **Cent** | Unit of pitch difference. 100 cents = 1 semitone. Used as the measure of discrimination precision. |
| **Reference Pitch** | The tuning standard used to derive all note frequencies. Default: A4 = 440Hz. Configurable in settings. |
| **Training Range** | The span of musical notes the app uses for comparisons. Can be set manually or expand adaptively based on performance at the edges. |
| **Cold Start** | The initial state for a new user: all notes are treated as weak, comparisons use 100-cent (1 semitone) differences, pairs are selected randomly. |
| **Weak Spot** | A note or region in the training range where the user's detectable cent difference is relatively large ‚Äî indicating lower discrimination ability. The algorithm prioritizes these. |
| **Natural vs. Mechanical** | The user-facing algorithm behavior slider. "Natural" keeps comparisons in nearby pitch regions before jumping to weak spots. "Mechanical" jumps more aggressively across the range. |
| **Confidence Band** | The visual representation of detection thresholds across the training range, overlaid on the piano keyboard in the perceptual profile visualization. |
| **Detection Threshold** | The smallest cent difference the user can reliably discriminate at a given note. Derived from comparison history. |

---

Anything to add, rename, or cut? Once you're happy with this, I'll fold both the glossary and the "note pairs ‚Üí comparisons" rename into the document and we'll continue to NFRs.

### üßë Human (09:35:01)

Yes, add entries for the elements of the app that a user interacts with. E.g., the start screen (or is it "start page"?) the comparison page, settings and the various buttons.

### ü§ñ Assistant (09:35:17)

Right ‚Äî let's nail down the UI vocabulary. Here are the screen and control terms. I need your input on a few naming decisions:

**Screens:**

First question: **"screen" or "page"?** In iOS convention, "screen" is more native (UIKit/SwiftUI think in views and screens). "Page" is more web. I'd go with **"screen"** for consistency with the platform. Agree?

**Draft UI glossary entries:**

| Term | Definition |
|---|---|
| **Start Screen** | The app's home screen. Shows the "Start Training" button, a short app description, a stylized perceptual profile preview (tappable), and a link to settings. |
| **Training Screen** | The active training interface. Shows the Higher/Lower buttons, a stop button, and visual feedback. Minimal by design ‚Äî no distractions. |
| **Profile Screen** | The full perceptual profile visualization. Shows the piano keyboard with confidence band overlay, summary statistics (mean and standard deviation with trend), accessed by tapping the profile preview on the Start Screen. |
| **Settings Screen** | Configuration interface. Contains the Natural vs. Mechanical slider, note range, note duration, reference pitch, and sound source selection. |
| **Start Training Button** | Prominent button on the Start Screen that immediately begins a training session. The primary call to action. |
| **Higher Button** | Training Screen control. User taps this when they judge the second note to be higher than the first. Disabled during the first note, enabled when the second note plays. |
| **Lower Button** | Training Screen control. User taps this when they judge the second note to be lower than the first. Disabled during the first note, enabled when the second note plays. |
| **Stop Button** | Icon-only button on the Training Screen that ends the current training session. Discards any incomplete comparison. No confirmation dialog. |
| **Profile Preview** | Stylized miniature of the perceptual profile shown on the Start Screen. Tappable to navigate to the full Profile Screen. |
| **Feedback Indicator** | Visual element on the Training Screen showing thumbs up (correct) or thumbs down (incorrect) after each comparison. Accompanied by haptic feedback on incorrect answers. |

---

A few things I want you to weigh in on:

1. **"Profile Screen"** ‚Äî or would you prefer **"Visualization Screen"**, **"Progress Screen"**, or something else? "Profile" ties it to the perceptual profile concept.
2. **"Higher Button" / "Lower Button"** ‚Äî or should these be a single entry like **"Answer Buttons"** with the two options described?
3. **"Feedback Indicator"** ‚Äî or is there a better name? This is the thumbs up/down visual.

What feels right?

### üßë Human (09:37:20)

I prefer "Profile Screen". The others are good.

### ü§ñ Assistant (09:37:30)

Good. Here's the complete glossary with everything consolidated:

---

## Glossary

### Concepts

| Term | Definition |
|---|---|
| **Comparison** | A single training interaction: two sequential notes played, user judges whether the second is higher or lower than the first. The atomic unit of training. |
| **Perceptual Profile** | A map of the user's pitch discrimination ability across their training range, showing the smallest detectable cent difference at each note. Visualized as a confidence band over a piano keyboard. |
| **Cent** | Unit of pitch difference. 100 cents = 1 semitone. Used as the measure of discrimination precision. |
| **Reference Pitch** | The tuning standard used to derive all note frequencies. Default: A4 = 440Hz. Configurable in settings. |
| **Training Range** | The span of musical notes the app uses for comparisons. Can be set manually or expand adaptively based on performance at the edges. |
| **Cold Start** | The initial state for a new user: all notes are treated as weak, comparisons use 100-cent (1 semitone) differences, pairs are selected randomly. |
| **Weak Spot** | A note or region in the training range where the user's detectable cent difference is relatively large ‚Äî indicating lower discrimination ability. The algorithm prioritizes these. |
| **Natural vs. Mechanical** | The user-facing algorithm behavior slider. "Natural" keeps comparisons in nearby pitch regions before jumping to weak spots. "Mechanical" jumps more aggressively across the range. |
| **Confidence Band** | The visual representation of detection thresholds across the training range, overlaid on the piano keyboard in the perceptual profile visualization. |
| **Detection Threshold** | The smallest cent difference the user can reliably discriminate at a given note. Derived from comparison history. |

### Screens

| Term | Definition |
|---|---|
| **Start Screen** | The app's home screen. Shows the Start Training Button, a short app description, the Profile Preview (tappable), and a link to the Settings Screen. |
| **Training Screen** | The active training interface. Shows the Higher Button, Lower Button, Stop Button, and Feedback Indicator. Minimal by design ‚Äî no distractions. |
| **Profile Screen** | The full perceptual profile visualization. Shows the piano keyboard with confidence band overlay, summary statistics (mean and standard deviation with trend). Accessed by tapping the Profile Preview on the Start Screen. |
| **Settings Screen** | Configuration interface. Contains the Natural vs. Mechanical slider, note range, note duration, reference pitch, and sound source selection. |

### Controls

| Term | Definition |
|---|---|
| **Start Training Button** | Prominent button on the Start Screen that immediately begins a training session. The primary call to action. |
| **Higher Button** | Training Screen control. User taps this when they judge the second note to be higher than the first. Disabled during the first note, enabled when the second note plays. |
| **Lower Button** | Training Screen control. User taps this when they judge the second note to be lower than the first. Disabled during the first note, enabled when the second note plays. |
| **Stop Button** | Icon-only button on the Training Screen that ends the current training session. Discards any incomplete comparison. No confirmation dialog. |
| **Profile Preview** | Stylized miniature of the perceptual profile shown on the Start Screen. Tappable to navigate to the full Profile Screen. |
| **Feedback Indicator** | Visual element on the Training Screen showing thumbs up (correct) or thumbs down (incorrect) after each comparison. Accompanied by haptic feedback on incorrect answers. |

---

I should also update the FRs and journeys to use "comparison" instead of "pair" consistently. I'll do that when saving. Ready to continue to NFRs, or anything else to adjust?

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Non-Functional Requirements (Step 10 of 11)

### üßë Human (09:38:16)

Before we continue, write the glossary to a suitably named file.

### ü§ñ Assistant (09:38:29)

Written to `docs/planning-artifacts/glossary.md`. 

Now ‚Äî ready to continue, or anything else before NFRs?

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Non-Functional Requirements (Step 10 of 11)

### üßë Human (09:38:56)

c

### ü§ñ Assistant (09:38:59)

Let me first update the FRs to use "comparison" consistently, then save and move on.

### ü§ñ Assistant (09:39:03)

Now I'll update the frontmatter, append the FRs (with "comparison" terminology), add a glossary reference, and update the journeys to use consistent terminology.

### ü§ñ Assistant (09:39:08)

Now let me update the journeys and scoping to use "comparison" consistently, and append the FRs.

### ü§ñ Assistant (09:39:32)

Now update the journeys:

### ü§ñ Assistant (09:40:12)

Now update the remaining "pair" references in the requirements summary and implementation sections:

### ü§ñ Assistant (09:40:27)

Now append the FRs and glossary reference:

### ü§ñ Assistant (09:40:59)

Done. FRs appended with consistent "comparison" terminology, glossary terms used for UI elements (Start Screen, Stop Button, Feedback Indicator, Profile Preview, Profile Screen), and a glossary cross-reference added. All prior sections updated to use "comparison" instead of "pair."

Now loading Step 10.

### ü§ñ Assistant (09:41:18)

**Progress: Step 10 of 11** ‚Äî Next: Polish Document

---

Michael, NFRs define *how well* the system performs, not *what* it does. For Peach, most standard NFR categories don't apply ‚Äî no backend means no scalability, no sensitive data means minimal security, no integrations. Let me focus on what actually matters:

**Relevant categories:**

1. **Performance** ‚Äî Yes. Audio latency is critical. If there's a perceptible delay between tapping "Higher" and the next comparison starting, the reflexive flow breaks. Also, note generation must be precise in frequency.

2. **Accessibility** ‚Äî Lightly. You've already scoped this to low-hanging fruit (labels, contrast, VoiceOver basics) in the FRs.

3. **Data Integrity** ‚Äî Yes. The training history is irreplaceable. If the app loses data, the perceptual profile is gone.

**Not relevant:** Security (no sensitive data, no auth, no network), Scalability (single-user, on-device), Integration (no external systems).

Let me draft what I think matters. Push back if I'm over- or under-specifying:

---

## Non-Functional Requirements

### Performance

- Audio latency: time from triggering a note to audible output must be imperceptible to the user (target < 10ms)
- Transition between comparisons: next comparison must begin immediately after the user answers ‚Äî no perceptible loading or delay
- Frequency precision: generated tones must be accurate to within 0.1 cent of the target frequency
- App launch to training-ready: Start Screen must be interactive within 2 seconds of app launch
- Profile Screen rendering: perceptual profile visualization must render within 1 second, including summary statistics computation

### Accessibility

- All interactive controls labeled for VoiceOver
- Sufficient color contrast ratios for all text and UI elements
- Tap targets meet minimum size guidelines (44x44 points per Apple HIG)
- Feedback Indicator provides non-visual feedback (haptic) in addition to visual

### Data Integrity

- Training data must survive app crashes, force quits, and unexpected termination without loss
- Data writes must be atomic ‚Äî no partial comparison records
- App updates must preserve all existing training data (no migration data loss)

---

Does this hit the right level? Anything to add or adjust ‚Äî particularly on the audio latency target?

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue to Polish Document (Step 11 of 11)

### üßë Human (09:42:52)

c

### ü§ñ Assistant (09:43:17)

Now loading Step 11: Polish Document.

### ü§ñ Assistant (09:43:26)

Good ‚Äî I've internalized the PRD purpose document. Now let me review the full PRD and polish it.

### ü§ñ Assistant (09:43:43)

**Progress: Step 11 of 11** ‚Äî Next: Complete PRD

---

Michael, I've reviewed the full document against the BMAD PRD purpose guidelines. Here's my polish assessment:

**What's working well:**
- Information density is high ‚Äî very little fluff
- FRs are capability-focused and implementation-agnostic
- NFRs are measurable with specific targets
- Consistent "comparison" terminology throughout
- Glossary cross-reference in place

**Issues I want to fix:**

1. **Missing Executive Summary** ‚Äî The document jumps straight into Success Criteria. It needs a concise overview: what Peach is, who it's for, and what makes it different. This is critical for downstream consumers (UX designer, architect) who need the 30-second picture.

2. **Journey 3 describes post-MVP features** ‚Äî It references temporal view and per-note detail tapping, which were moved to Phase 2. The journey is still valuable as a vision piece, but it should be annotated to clarify which elements are MVP vs. post-MVP.

3. **Journey Requirements Summary has a stale entry** ‚Äî "Temporal progress visualization" and "Per-note detail view" are listed as MVP, but scoping moved these to Phase 2. The summary stats trend replaced them.

4. **Success Criteria has a stale entry** ‚Äî "The temporal visualization makes improvement visible without reducing it to a single score" contradicts the scoping decision to use mean + standard deviation for MVP.

5. **Minor duplication** ‚Äî "Mobile App Specific Requirements" repeats some information already captured in FRs (localization, audio interruption handling, data persistence). I'll consolidate by removing duplicated specifics and keeping the section as platform context.

**Proposed changes:**
- Add an Executive Summary section at the top
- Fix stale references to match scoping decisions
- Annotate Journey 3 for MVP vs. post-MVP elements
- Update Journey Requirements Summary
- Trim duplication in Mobile App Specific Requirements

Shall I proceed with these changes, or do you want to adjust the plan?

**Select:** [A] Advanced Elicitation [P] Party Mode [C] Continue ‚Äî apply polish and move to completion

### üßë Human (09:44:18)

c

### ü§ñ Assistant (09:45:50)

Polished document saved. Now loading the final step.

### ü§ñ Assistant (09:46:25)

Michael, the PRD for Peach is **done**.

**What we built:**
- **Executive Summary** ‚Äî vision, target users, design philosophy, technology
- **Success Criteria** ‚Äî user, business, technical, measurable outcomes
- **Project Scoping** ‚Äî MVP/Growth/Vision phases with risk mitigation
- **User Journeys** ‚Äî 5 narrative journeys covering all key scenarios
- **Mobile App Requirements** ‚Äî platform, device, data, App Store, architecture
- **Functional Requirements** ‚Äî 42 FRs across 8 capability areas (the capability contract)
- **Non-Functional Requirements** ‚Äî performance, accessibility, data integrity
- **Glossary** ‚Äî separate file with all product terms (concepts, screens, controls)

**Output files:**
- `docs/planning-artifacts/prd.md`
- `docs/planning-artifacts/glossary.md`

---

**What's next?** Here are the recommended next steps, each in a fresh context window:

1. **Validate PRD** (optional but recommended)
   `/bmad-agent-bmm-pm` ‚Üí then select VP (Validate PRD)
   Agent: John, Product Manager
   *Recommend using a different LLM for validation if available.*

2. **Create UX Design** (optional, recommended for UI-heavy apps like Peach)
   `/bmad-bmm-create-ux-design`
   Agent: Sally, UX Designer
   *Translates your journeys and FRs into interaction design.*

3. **Create Architecture** (required before epics)
   `/bmad-bmm-create-architecture`
   Agent: Winston, Architect
   *Technical decisions: data model, audio engine, algorithm structure.*

4. **Create Epics & Stories** (required, after architecture)
   `/bmad-agent-bmm-pm` ‚Üí then select CE (Create Epics and Stories)
   Agent: John, Product Manager
   *Breaks the FRs into implementable work units.*

5. **Check Implementation Readiness** (required, before development)
   `/bmad-agent-bmm-pm` ‚Üí then select IR (Implementation Readiness)
   Agent: Winston, Architect
   *Ensures PRD, UX, Architecture, and Epics are aligned.*

The PRD is now the foundation ‚Äî all downstream design, architecture, and development work traces back to it. Update it as needed as you continue planning.

### üßë Human (09:50:14)

Add all changes to git with a meaningful commit message. Don't forget the glossary.

### ü§ñ Assistant (09:50:44)

Committed. Both `prd.md` and `glossary.md` are in `7bb2a2e`.

### üßë Human (09:51:10)

<command-name>/exit</command-name>
            <command-message>exit</command-message>
            <command-args></command-args>

### üßë Human (09:51:10)

<local-command-stdout>Catch you later!</local-command-stdout>

---

[‚Üê Previous (0a89329199)](0a89329199.md) | [Index](index.md) | [Next (7c1eb9b60e) ‚Üí](7c1eb9b60e.md)
